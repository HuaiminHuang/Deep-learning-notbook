{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b97207",
   "metadata": {},
   "source": [
    "# 从rl的PPO到RL4HF中的PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70a95b",
   "metadata": {},
   "source": [
    "## 1. 经典 PPO-Clip 公式 (Schulman et al., 2017)\n",
    "\n",
    "在 OpenAI 提出的 PPO 原始论文 “Proximal Policy Optimization Algorithms” (Schulman et al., 2017) 里，目标函数是：\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
    "- $A_t$ 是优势函数\n",
    "- $\\epsilon$ 是超参数 (如 0.1~0.2)\n",
    "\n",
    "特点：\n",
    "\n",
    "- 状态 $s_t$，动作 $a_t$：典型 RL 表达方式\n",
    "\n",
    "- 没有序列长度归一化\n",
    "\n",
    "- 没有语言建模的上下文结构，动作是单步离散的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb36d6",
   "metadata": {},
   "source": [
    "## 关键改动点：\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text{old}}}(O|q)} \\left[ \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left( \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{<t})} A_t, \\ \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "- **状态替换**:  \n",
    "  $s_t \\rightarrow (q, o_{<t})$\n",
    "  - 在语言建模中，状态由输入 prompt 和前缀 token 构成。\n",
    "\n",
    "- **动作替换**:  \n",
    "  - $a_t \\rightarrow \\text{token } o_t$。\n",
    "\n",
    "- **轨迹归一化**:  \n",
    "  - 外层有 $\\frac{1}{|o|} \\sum_t$，表示对一个生成序列内的每个 token 平均，而不是直接累积。\n",
    "\n",
    "- **采样分布**:  \n",
    "  - $\\mathbb{E}_{q \\sim P(Q)}$ 表示从 prompt 分布中抽样（例如 RLHF 数据集中的问题）。\n",
    "\n",
    "- **概率分布**:  \n",
    "  - 用的是 语言模型条件概率 $\\pi_\\theta(o_t|q, o_{<t})$，而不是单步 MDP 动作概率。\n",
    "\n",
    "\n",
    "论文：\n",
    "\n",
    "“Training language models to follow instructions with human feedback” (OpenAI, 2022)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfe9e2",
   "metadata": {},
   "source": [
    "### 策略目标(objective)\n",
    "论文给出的公式为：\n",
    "$$\n",
    "\\text{objective}(\\phi) = \\mathbb{E}_{(x,y) \\sim D_{\\pi_{\\phi}^{RL}}} \\left[ r_\\theta(x, y) - \\beta \\log \\frac{\\pi_\\phi^{RL}(y \\mid x)}{\\pi^{SFT}(y \\mid x)} \\right] + \\gamma \\mathbb{E}_{x \\sim D_{\\text{pretrain}}} \\left[ \\log \\pi_\\phi^{RL}(x) \\right]\n",
    "$$\n",
    "说明：  \n",
    "- 最大化reward，但是使用两种方式进行校准拉回。\n",
    "  - 第一种是KL的罚项，$- \\beta \\log \\frac{\\pi_\\phi^{RL}(y \\mid x)}{\\pi^{SFT}(y \\mid x)}$。惩罚离开SFT太远，$\\beta$ 是权重越大越保守，越小越最求reward。\n",
    "  - $\\gamma \\mathbb{E}_{x \\sim D_{\\text{pretrain}}} \\left[ \\log \\pi_\\phi^{RL}(x) \\right]$是为了保持语言模型的基本能力/避免遗忘。保证语言模型的流畅度，防止过分最求reward。\n",
    "- $D_{\\pi_{\\phi}^{RL}}$: 按照当前策略采样得到的偏好对(x, y)的分布（先从promt抽出x,然后再从当前策略 $\\pi_{\\phi}^{RL}$ 生成y）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523e5f2",
   "metadata": {},
   "source": [
    "### 奖励模型的loss\n",
    "\n",
    "$$\n",
    "\\text{loss}(\\theta) \\ = \\ -\\frac{1}{(K/2)} \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\left( \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l)) \\right) \\right]\n",
    "$$\n",
    "说明：\n",
    "- 数据$D$是偏好数据集：对用一种问题prompt-x，有一对回答 $(y_w, y_l)$ winner和loser。\n",
    "- $\\sigma(\\cdot)$；$sigmoid()$ 函数\n",
    "- $-1/(K/2)$ 只是归一化，这里K是总比较数\n",
    "- 实际操作里，常常把KL散度放到reward里面保持稳定更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b28bc",
   "metadata": {},
   "source": [
    "# 总结\n",
    "- 最初的 PPO = 环境 reward + ratio-clip 稳定更新\n",
    "- RLHF 版本的 PPO = 奖励模型打分 + 显式 KL 惩罚 + 语言建模正则，本质还是 PPO，但为了文本生成和人类偏好场景做了定制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad7b4f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
