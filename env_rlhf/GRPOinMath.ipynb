{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca06d211",
   "metadata": {},
   "source": [
    "# GRPO \n",
    "\n",
    "[deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1)\n",
    "\n",
    "[HF:GRPO Trainer](https://huggingface.co/docs/trl/grpo_trainer)\n",
    "\n",
    "[Approximating KL Divergence](http://joschu.net/blog/kl-approx.html)\n",
    "\n",
    "[Deepseek math](https://arxiv.org/abs/2402.03300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b80c9",
   "metadata": {},
   "source": [
    "## Why GRPO\n",
    "1. PPO $\\rightarrow$ DPO $\\rightarrow$ GRPO \n",
    "    - ppo 需要单独训练一个reward model 进行训练，架构复杂且算力需求量大\n",
    "      - 目标：\n",
    "        $$\n",
    "        \\max_{\\theta} \\mathbb{E}_{(x,y) \\sim \\pi_{\\theta}} \\left[ r_{\\phi}(x,y) - \\beta \\, \\text{KL}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) \\right]\n",
    "        $$\n",
    "    - DPO 在 PPO 的基础上改进了 reward model 写出其显示解，使用偏好数据对(chosen, rejucted)：\n",
    "      - $$\n",
    "        r(x, y) \\approx \\beta \\left( \\log \\pi_{\\theta}(y|x) - \\log \\pi_{\\text{ref}}(y|x) \\right)\n",
    "        $$\n",
    "        - 这样就显式地写出损失函数：\n",
    "        $$\n",
    "        \\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}_{(x, y^+, y^-)} \\left[ \\log \\sigma \\left( \\beta (\\Delta_{\\theta} - \\Delta_{\\text{ref}}) \\right) \\right]\n",
    "        $$  \n",
    "         - 其中 $\\Delta_{\\theta} = \\log \\pi_{\\theta}(y^+|x) - \\log \\pi_{\\theta}(y^-|x)$。\n",
    "    - GRPO ：在复杂的任务，一个 prompt 通常会有多个候选答案，不只是但一个评判标准--好/坏\n",
    "      - 主要改进在reward：\n",
    "        - 把多个候选作为一组（group），组内进行比较。\n",
    "        - 同样不训练 critic ，不需要显示的reward model，使用**组内的相对评分**用来估计优势函数\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6e904",
   "metadata": {},
   "source": [
    "## GRPO具体流程\n",
    "对于每一个question q，GRPO从旧的策略 $\\pi_{\\theta_{old}}$ 采样一组输出 ${o}_i^{G} = \\{o_1, o_2, ..., o_G \\} $，并且通过最大化下面的目标：\n",
    "$$\n",
    "J_{G R P O}(\\theta)=\\mathbb{E}[q \\sim P(Q),\\{o_{i}\\}_{i=1}^{G} \\sim \\pi_{\\theta_{o l d}}(O|q)]\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{|o_{i}|} \\sum_{t=1}^{|o_{i}|}\\left\\{\\min \\left[\\frac{\\pi_{\\theta}(o_{i, t}|q, o_{i,<t})}{\\pi_{\\theta_{o l d}}(o_{i, t}|q, o_{i,<t})} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}(o_{i, t}|q, o_{i,<t})}{\\pi_{\\theta_{o l d}}(o_{i, t}|q, o_{i,<t})}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathrm{D}_{\\mathrm{KL}}\\left[\\pi_{\\theta} \\| \\pi_{\\mathrm{ref}}\\right]\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbb62b",
   "metadata": {},
   "source": [
    "![PPO and GRPO](./img/PPO-and-GRPO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce16c19",
   "metadata": {},
   "source": [
    "2. GRPO 流程（下半部分）\n",
    "\n",
    "GRPO 的改进在于 **去掉** Value Model，用 **组内比较**(Group-based Advantage Estimation) 代替基线。\n",
    "\n",
    "- **Step 1: 输入** \\( q \\)\n",
    "  - Policy Model 根据 prompt \\( q \\) 生成一组候选输出：\n",
    "    $$\n",
    "    o_1, o_2, \\ldots, o_G\n",
    "    $$\n",
    "- **Step 2: 评估**\n",
    "  - 每个输出通过 Reference Model 计算 KL 惩罚。\n",
    "  - 每个输出通过 Reward Model 得到奖励分数：\n",
    "    $$ r_1, r_2, \\ldots, r_G$$\n",
    "\n",
    "- **Step 3: Group Computation**\n",
    "  - 不再用 Value Model 来估计 baseline，而是利用 **组内统计量**（如平均分或 pairwise 比较）来估计基线。\n",
    "  - 对每个输出计算 advantage：\n",
    "    $$ A_i = r_i - \\text{baseline(group)}$$\n",
    "    - baseline(group) 可以是组内平均奖励，也可以是其他相对评分方式。\n",
    "\n",
    "- **Step 4: Policy 更新**\n",
    "  - 使用 PPO 的 clipped objective，但 advantage 来自 **组内相对优势**，而非 value network。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008ef15",
   "metadata": {},
   "source": [
    "![GRPO method](img/GRPO-method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7896a8",
   "metadata": {},
   "source": [
    "\n",
    "![algorithm](./img/GRPO-Algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506288e5",
   "metadata": {},
   "source": [
    "## More detials\n",
    "### 1.Outcome Supervision RL with GRPO\n",
    "\n",
    "对于一个输入问题 \\( q \\)，采样 \\( G \\) 个输出：\n",
    "$$\n",
    "\\{ o_1, o_2, \\ldots, o_G \\}\n",
    "$$\n",
    "Reward model 给出对应奖励：\n",
    "$$\n",
    "r = \\{ r_1, r_2, \\ldots, r_G \\}\n",
    "$$\n",
    "然后做组内归一化（z-score）：\n",
    "$$\n",
    "\\hat{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}\n",
    "$$\n",
    "这里的 baseline 就是组平均 \\(\\text{mean}(r)\\)，同时再除以 \\(\\text{std}(r)\\) 做标准化，避免 reward scale 不稳定。\n",
    "最后所有 token 的优势函数：\n",
    "$$\n",
    "\\hat{A}_{i, t} = \\hat{r}_i\n",
    "$$\n",
    "\n",
    "### 2.Process Supervision RL with GRPO\n",
    "\n",
    "区别在于 **奖励不是只有最终答案**，而是对中间每个 reasoning step 都打分。\n",
    "\n",
    "假设第 \\( i \\) 个输出有 \\( K_i \\) 步，每一步的奖励是：\n",
    "\n",
    "$$\n",
    "r_i^{\\text{index}(j)} \\quad (j = 1, 2, \\ldots, K_i)\n",
    "$$\n",
    "\n",
    "组内 baseline 的计算方式同样是：\n",
    "\n",
    "$$\n",
    "\\tilde{r}_i^{\\text{index}(j)} = \\frac{r_i^{\\text{index}(j)} - \\text{mean}(R)}{\\text{std}(R)}\n",
    "$$\n",
    "\n",
    "其中 \\( R \\) 是所有 group 中所有 step 的 reward 集合。\n",
    "\n",
    "优势函数计算：\n",
    "\n",
    "$$\n",
    "\\hat{A}_{i, t} = \\sum_{\\text{index}(j) \\geq t} \\tilde{r}_i^{\\text{index}(j)}\n",
    "$$\n",
    "\n",
    "直观理解：\n",
    "\n",
    "- 每一步推理都会得到一个 reward，避免只在最后一步监督；\n",
    "- baseline 仍然是组平均 + 标准化，只不过是在 step-level。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d06ff2",
   "metadata": {},
   "source": [
    "### 3.KL散度\n",
    "\n",
    "$$\n",
    "\\mathbb{D}_{KL} \\left[ \\pi_{\\theta} \\| \\pi_{\\text{ref}} \\right] = \\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})} - 1,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e16d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
