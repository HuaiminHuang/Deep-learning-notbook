{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6332e755",
   "metadata": {},
   "source": [
    "# Transformers 架构\n",
    "\n",
    "一个单文件、可运行的 PyTorch 实现：接近现代 decoder-only Transformer（GPT 风格），  \n",
    "包含关键现代改进与工程要点：  \n",
    " - 多头自注意力（scaled dot-product）\n",
    " - Rotary positional embeddings(RoPE)（可选）\n",
    " - Gated Feed-Forward（GEGLU）\n",
    " - LayerNorm + Residual 结构\n",
    " - Causal mask（自回归）\n",
    " - Mixed precision training (AMP)\n",
    " - Activation checkpointing（可选）\n",
    " - 学习率线性 warmup -> cosine decay\n",
    " - 简单训练循环（含 gradient accumulation 与 checkpoint 保存）\n",
    " - 采样（top-k / top-p）用于生成\n",
    "\n",
    "设计目标：可读、现代化、易修改，注释突出关键细节以便教学与工程化扩展。\n",
    "\n",
    "注意：此实现为教学/原型级别。生产环境请使用经过高度优化的内核（FlashAttention、xFormers）、分布式并行与高效 IO。\n",
    "\n",
    "运行示例：\n",
    "  python modern_transformer.py --device cuda --epochs 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2662f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e68a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def gelu_new(x):\n",
    "    # GELU approximation that's widely used\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x ** 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e6f37",
   "metadata": {},
   "source": [
    "## 旋转位置编码（rope）\n",
    "RoPE 的灵感来源于复数旋转。在复数空间中，用一个复数 $e^{im\\theta_i}$ 去乘另一个复数 $(q_j + i q_{j+1})$，等价于将其旋转 $m\\theta_i$ 弧度。  \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   \\cos{(m\\theta_i)} & -\\sin{(m\\theta_i)} \\\\\n",
    "   \\sin{(m\\theta_i)} & \\cos{(m\\theta_i)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "所以，对于一对元素$[x_1, x_2]$旋转后的结果是：\n",
    "$$\n",
    "x_{1_rotated} = x_1 \\cos{(m\\theta_i)} - x_2 \\sin{(m\\theta_i)}\\\\\n",
    "x_{2_rotated} = x_1 \\sin{(m\\theta_i)} + x_2 \\cos{(m\\theta_i)}\n",
    "$$\n",
    "代码中的 `x * cos + x_rotated * sin` 正是以向量化且高效的方式实现了上述两个公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7866a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Rotary Positional Embeddings (RoPE)\n",
    "# -----------------------------\n",
    "# RoPE provides a way to inject relative-position bias into the attention by rotating query/key vectors.\n",
    "# It has become very common in modern decoder-only LLMs (GPT-NeoX, etc.).\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        # dim is the per-head dim (d_head). We create cos/sin caches lazily.\n",
    "        # 计算频率的倒数： theta_i = 10000^(-2i/dim)\n",
    "        # 我们要把 dim 维的空间分成 dim/2 (视作复数i的两个维度)个二维子空间(步长为2)\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq) #注册为不参与训练的缓冲区域\n",
    "        self._seq_len_cached = 0 # 记录当前缓存的长度\n",
    "\n",
    "    def _build_cache(self, seq_len, device):\n",
    "        # 如果超过缓存则跳过\n",
    "        if seq_len == self._seq_len_cached:\n",
    "            return\n",
    "        \n",
    "        # 位置索引 m: [0, 1, 2, ..., seq_len-1]\n",
    "        t = torch.arange(seq_len, device=device).type(self.inv_freq.dtype)\n",
    "    \n",
    "        # 外积: [seq_len] , [dim/2] -> [seq_len, dim/2]\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)  # [seq_len, dim/2]\n",
    "\n",
    "        # 对dim/2复制，[seq_len, dim/2] -> [seq_len, dim]\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # duplicate for sin/cos pairing m*theta_i\n",
    "        cos = emb.cos()[None, None, :, :]  # shape [1,1,seq_len,dim]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "\n",
    "        # 缓存起来\n",
    "        self.register_buffer('cos_cached', cos)\n",
    "        self.register_buffer('sin_cached', sin)\n",
    "        self._seq_len_cached = seq_len\n",
    "\n",
    "    def forward(self, x, seq_dim=-2):\n",
    "        # x是query 或者 key 向量 shape is [batch_size, n_head, seq_len, head_dim(dim))]\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        device = x.device\n",
    "        self._build_cache(seq_len, device)\n",
    "        return self.cos_cached[..., :seq_len, :], self.sin_cached[..., :seq_len, :]\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # x: [batch, n_head, seq_len, d_head]\n",
    "    # 分成奇数和偶数两组\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "\n",
    "    # 应用旋转嵌入公式：x_rotated = x * cos + x_rotated * sin\n",
    "    # 这等价于对 x 的每一对元素 [x_i, x_{i+1}] 进行了一个旋转矩阵操作\n",
    "    x_rotated = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rotated * sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2331352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Multi-head attention (with optional fused/flash path if available)\n",
    "# -----------------------------\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, attn_dropout=0.0, causal=True, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, 'd_model must be divisible by n_head'\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.causal = causal\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        # We project to QKV in one linear for efficiency\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "        if use_rope:\n",
    "            # Rotatory embeddings per head dimension\n",
    "            self.rotary = RotaryEmbedding(self.d_head)\n",
    "        else:\n",
    "            self.rotary = None\n",
    "\n",
    "    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n",
    "        # x: [B, T, d_model] == [批次大小，序列长度，模型维度]\n",
    "        B, T, _ = x.shape\n",
    "        qkv = self.qkv(x)  # [B, T, 3*d]\n",
    "        \n",
    "        # reshape+转置： [B, T, 3, n_head, d_head] ->  [3, B, n_head, T, d_head]\n",
    "        qkv = qkv.view(B, T, 3, self.n_head, self.d_head).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # 取出合并对应的q k v\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: [B, n_head, T, d_head]\n",
    "\n",
    "\n",
    "        # 是否使用旋转位置编码 shape 不变\n",
    "        if self.use_rope and self.rotary is not None:\n",
    "            cos, sin = self.rotary(q)\n",
    "            q = apply_rotary_pos_emb(q, cos, sin)\n",
    "            k = apply_rotary_pos_emb(k, cos, sin)\n",
    "\n",
    "        # compute scaled dot-product attention\n",
    "        # scores: [B, n_head, T, T]-->\n",
    "        # --> [B, n_head, T, d_head] × [B, n_head, d_head, T] = [B, n_head, T, T]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "\n",
    "        # causal mask: prevent attending to future tokens in decoder-only models\n",
    "        # 因果掩码: [1, 1, T, T]\n",
    "        # causal_mask 现在是一个 [T, T] 的矩阵，看起来像这样（T=4）：\n",
    "        # [[ True, False, False, False],\n",
    "        #  [ True,  True, False, False],\n",
    "        #  [ True,  True,  True, False],\n",
    "        #  [ True,  True,  True, True]]\n",
    "        if self.causal:\n",
    "            causal_mask = torch.tril(torch.ones((T, T), device=x.device, dtype=torch.bool)).view(1, 1, T, T)\n",
    "            scores = scores.masked_fill(~causal_mask, float('-inf'))\n",
    "\n",
    "        # external attention mask (e.g., padding masks)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask expected broadcasting to [B, 1, 1, T] or [B, 1, T, T]\n",
    "            scores = scores + attn_mask\n",
    "\n",
    "        # 标准注意力计算\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        out = torch.matmul(attn, v)  # [B, n_head, T, d_head]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f7b9b",
   "metadata": {},
   "source": [
    "这里的多头注意力可以改进为GQA\n",
    "```python\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, n_kv_heads=None, attn_dropout=0.0, causal=True, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, 'd_model must be divisible by n_head'\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.causal = causal\n",
    "        self.use_rope = use_rope\n",
    "        \n",
    "        # 设置KV头的数量（默认为查询头数的1/4或1/8）\n",
    "        self.n_kv_heads = n_kv_heads if n_kv_heads is not None else max(1, n_head // 8)\n",
    "        assert n_head % self.n_kv_heads == 0, 'n_head must be divisible by n_kv_heads'\n",
    "        self.n_rep = n_head // self.n_kv_heads  # 每个KV头重复的次数\n",
    "        \n",
    "        # 分开的Q、K、V投影\n",
    "        self.q_proj = nn.Linear(d_model, n_head * self.d_head)\n",
    "        self.k_proj = nn.Linear(d_model, self.n_kv_heads * self.d_head)\n",
    "        self.v_proj = nn.Linear(d_model, self.n_kv_heads * self.d_head)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        \n",
    "        if use_rope:\n",
    "            self.rotary = RotaryEmbedding(self.d_head)\n",
    "        else:\n",
    "            self.rotary = None\n",
    "    \n",
    "    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n",
    "        B, T, _ = x.shape\n",
    "        \n",
    "        # 分别计算Q、K、V\n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.d_head).transpose(1, 2)  # [B, n_head, T, d_head]\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.d_head).transpose(1, 2)  # [B, n_kv_heads, T, d_head]\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.d_head).transpose(1, 2)  # [B, n_kv_heads, T, d_head]\n",
    "        \n",
    "        # 应用旋转位置编码\n",
    "        if self.use_rope and self.rotary is not None:\n",
    "            cos, sin = self.rotary(q)\n",
    "            q = apply_rotary_pos_emb(q, cos, sin)\n",
    "            k = apply_rotary_pos_emb(k, cos, sin)\n",
    "        \n",
    "        # 重复K和V以匹配Q的头数\n",
    "        k = k.repeat_interleave(self.n_rep, dim=1)  # [B, n_head, T, d_head]\n",
    "        v = v.repeat_interleave(self.n_rep, dim=1)  # [B, n_head, T, d_head]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        # 应用因果掩码\n",
    "        if self.causal:\n",
    "            causal_mask = torch.tril(torch.ones((T, T), device=x.device, dtype=torch.bool)).view(1, 1, T, T)\n",
    "            scores = scores.masked_fill(~causal_mask, float('-inf'))\n",
    "        \n",
    "        # 应用外部注意力掩码\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # 应用注意力权重到值向量\n",
    "        out = torch.matmul(attn, v)  # [B, n_head, T, d_head]\n",
    "        \n",
    "        # 合并多头输出\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        \n",
    "        # 最终输出投影\n",
    "        return self.out(out)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcedb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Feed-Forward (GEGLU) - gated linear unit variant that often performs better\n",
    "# -----------------------------\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # GEGLU: gate = W2(x) * GELU(W1(x)) but implemented as two linear layers\n",
    "        self.w1 = nn.Linear(d_model, d_ff)   # 升维: d_model -> d_ff\n",
    "        self.w2 = nn.Linear(d_model, d_ff)   # 另一个升维路径（用于门控）\n",
    "        self.proj = nn.Linear(d_ff, d_model) # 降维/投影\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, d_model]\n",
    "        return self.proj(self.dropout(gelu_new(self.w1(x)) * self.w2(x)))\n",
    "        #        ^^^^^^                          ^^               ^^\n",
    "        #    投影回d_model                       d_ff             d_ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a4295",
   "metadata": {},
   "source": [
    "这里的GEGLU可以切换为swishGLU，优化硬件提高效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Transformer Block\n",
    "# -----------------------------\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1, \n",
    "                 attn_dropout=0.0, use_rope=True, checkpoint=False):\n",
    "        super().__init__()\n",
    "        # Pre-LN architecture is the modern default for stability for deep stacks\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, n_head, attn_dropout=attn_dropout, \n",
    "                                           causal=True, use_rope=use_rope)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Pre-LN then residual\n",
    "        # (标准的是post-LN)\n",
    "        def _attn_forward(x, attn_mask):\n",
    "            return self.attn(self.ln1(x), attn_mask)\n",
    "\n",
    "        attn_out = _attn_forward(x, attn_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        ff_out = self.ff(self.ln2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35699f0e",
   "metadata": {},
   "source": [
    "![Traditional Transformers](img/transformers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0f32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Decoder-only Transformer (stack of blocks) + final lm head\n",
    "# -----------------------------\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, n_layer=12, n_head=12, d_ff=3072, max_seq_len=1024,\n",
    "                 dropout=0.1, attn_dropout=0.0, use_rope=True, checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 首先把 token_id 映射到连续的向量空间，embeding 维度 = d_model\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 位置编码矩阵 [1, T, d]\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq_len, d_model))  # optional learned pos\n",
    "        \n",
    "        # 堆叠 n_layer 个TransformerBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_head, d_ff, dropout=dropout, \n",
    "                             attn_dropout=attn_dropout, use_rope=use_rope, checkpoint=checkpoint)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "        # weight tying between token_emb and lm_head is common \n",
    "        # (improves sample quality)\n",
    "        # 把隐藏层映射回词表大小，得到logits\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_emb.weight  # 权重共享减少参数量，经典论文：Press & Wolf, 2016\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids, attn_mask=None):\n",
    "        # input_ids: [B, T]\n",
    "        B, T = input_ids.shape\n",
    "        assert T <= self.max_seq_len, 'sequence length exceeds model maximum'\n",
    "\n",
    "        x = self.token_emb(input_ids)  # [B, T, d]\n",
    "        # add learned positional embeddings (alternative: use RoPE only)\n",
    "        x = x + self.pos_emb[:, :T, :]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask=attn_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) \n",
    "        return logits\n",
    "\n",
    "    # 生成函数，推理不计算梯度\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=100, device=None, temperature=1.0, top_k=0, top_p=0.0):\n",
    "        # Simple autoregressive sampling. Not optimized for long sequences.\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "\n",
    "        B, T = input_ids.shape\n",
    "        out = input_ids\n",
    "        for _ in range(max_new_tokens):\n",
    "            if out.shape[1] > self.max_seq_len:\n",
    "                inp = out[:, -self.max_seq_len:]  # 只保留最近 max_seq_len 个\n",
    "            else:\n",
    "                inp = out\n",
    "            logits = self.forward(inp)\n",
    "            # temperature：softmax 平滑控制（>1 更随机，<1 更确定）\n",
    "            next_logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "\n",
    "            # Top-k filtering\n",
    "            # 只保留概率前 k 个 token，其余置为 -Inf\n",
    "            if top_k > 0:\n",
    "                vals, _ = torch.topk(next_logits, top_k)\n",
    "                min_vals = vals[:, -1].unsqueeze(1)\n",
    "                next_logits = torch.where(next_logits < min_vals, torch.full_like(next_logits, -float('Inf')), next_logits)\n",
    "\n",
    "            # Top-p (nucleus) filtering\n",
    "            if top_p > 0.0:\n",
    "                # 按概率排序，保留累计概率 ≤ top_p 的 token（常设置为0.9）\n",
    "                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
    "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "                # shift right to keep first above-threshold token\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                next_logits[..., indices_to_remove] = -float('Inf')\n",
    "            \n",
    "            # 归一化概率后采样一个token\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            out = torch.cat([out, next_token], dim=1)  # [B, T + max_new_tokens]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f50005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Mini dataset for language modeling (character-level or small subword)\n",
    "# -----------------------------\n",
    "\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        # texts: list of str; tokenizer: simple function mapping str->list[int]\n",
    "        # seq_len: 每个训练样本的最大长度\n",
    "        self.examples = []\n",
    "        for t in texts:\n",
    "            ids = tokenizer(t)\n",
    "\n",
    "            # break into blocks\n",
    "            for i in range(0, max(1, len(ids) - 1), seq_len):\n",
    "                block = ids[i:i + seq_len]\n",
    "                if len(block) < 2:\n",
    "                    continue\n",
    "\n",
    "                # 把 token 序列存成 torch.Tensor，方便后续训练。\n",
    "                # 所有块存到 self.examples 里。\n",
    "                self.examples.append(torch.tensor(block, dtype=torch.long))\n",
    "\n",
    "    # 必要实现\n",
    "    # 返回长度和一个样本\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716cd6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "tensor([104, 101, 108, 108, 111])\n"
     ]
    }
   ],
   "source": [
    "# 举个例子：\n",
    "texts = [\"hello world\", \"transformers are great\"]\n",
    "tokenizer = lambda x: [ord(c) for c in x]  # 简单用ASCII码作为tokenizer\n",
    "\n",
    "ds = SimpleTextDataset(texts, tokenizer, seq_len=5)\n",
    "print(len(ds))       # 多少个样本\n",
    "print(ds[0])         # 第一个样本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a10d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Simple tokenizer (byte-level BPE is heavy, we provide a toy tokenizer for demo)\n",
    "# In practical use, use Hugging Face tokenizers / SentencePiece / BPE.\n",
    "# -----------------------------\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, texts):\n",
    "        # build char-level vocab\n",
    "        chars = sorted(list(set(''.join(texts))))\n",
    "\n",
    "        # reserve 0 for padding\n",
    "        # 从 字符 --> 整数id 的映射，而外加一个<unk>处理此词表以外的字符\n",
    "        self.stoi = {c: i + 1 for i, c in enumerate(chars)}  \n",
    "        self.stoi['<unk>'] = len(self.stoi) + 1\n",
    "        # 逆运算itos\n",
    "        self.itos = {i: s for s, i in self.stoi.items()}\n",
    "        self.vocab_size = len(self.stoi) + 1\n",
    "\n",
    "    # 遍历str寻找id\n",
    "    def encode(self, text):\n",
    "        return [self.stoi.get(c, self.stoi['<unk>']) for c in text]\n",
    "\n",
    "    # 解码id查字符str\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.itos.get(i, '?') for i in ids])\n",
    "    \n",
    "    # 可以直接使用：tokenizer(\"hello\") 等价于 tokenizer.encode(\"hello\")\n",
    "    def __call__(self, text):\n",
    "        return self.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15055f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 9\n",
      "编码: [3, 2, 4, 4, 5]\n",
      "解码: hello\n"
     ]
    }
   ],
   "source": [
    "texts = [\"hello\", \"world\"]\n",
    "tokenizer = CharTokenizer(texts)\n",
    "\n",
    "print(\"词表大小:\", tokenizer.vocab_size)\n",
    "print(\"编码:\", tokenizer.encode(\"hello\"))\n",
    "print(\"解码:\", tokenizer.decode(tokenizer.encode(\"hello\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4dd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training utilities: optimizer, scheduler, save/load\n",
    "# -----------------------------\n",
    "\n",
    "# 训练时动态调整学习率\n",
    "class WarmupCosineScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps    # 前多少步用 线性 warmup\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.step_num = 0                   # 当前的训练步数\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        if self.step_num < self.warmup_steps:\n",
    "            # Warmup 阶段\n",
    "            lr_mult = float(self.step_num) / float(max(1, self.warmup_steps))\n",
    "        else:\n",
    "            # Cosine Decay 阶段\n",
    "            # 学习率从 initial_lr 逐渐下降到 0，呈余弦曲线。\n",
    "            progress = float(self.step_num - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n",
    "            lr_mult = max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = max(self.min_lr, param_group['initial_lr'] * lr_mult) \n",
    "\n",
    "# 保存模型\n",
    "def save_checkpoint(model, optimizer, step, path='./checkpoint/checkpoint.pt'):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'step': step\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac405e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "\n",
    "def train(model, dataset, device='cuda', epochs=1, batch_size=8, seq_len=128, lr=2e-4, weight_decay=0.01,\n",
    "          warmup_steps=100, total_steps=1000, grad_accum_steps=1, save_every=500):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # batch: list of 1D tensors of variable length -> pad to max_len\n",
    "        # 对于batch内的样本pad 到统一长度\n",
    "        max_len = max([b.size(0) for b in batch])\n",
    "        max_len = min(max_len, seq_len)\n",
    "        input_ids = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
    "        for i, b in enumerate(batch):\n",
    "            l = min(b.size(0), max_len)\n",
    "            input_ids[i, :l] = b[:l]\n",
    "        return input_ids  # shape [batch_size, seq_len]\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    for g in optimizer.param_groups:\n",
    "        g.setdefault('initial_lr', g['lr'])\n",
    "\n",
    "    scheduler = WarmupCosineScheduler(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # 用于混合精度训练（AMP），节省显存、加速训练\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # 主循环\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for batch in loader:\n",
    "            input_ids = batch.to(device)\n",
    "            # For language modeling, inputs and labels are the same but shifted.\n",
    "            # Simple approach: predict next token for each position -> labels = input_ids (shift internally)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids)\n",
    "                # shift logits and labels for next-token prediction\n",
    "                # 向右移动标签（滑动一位）:\n",
    "                # input = [a, b, c]  / tabels = [b, c, d]\n",
    "                shift_logits = logits[:, :-1, :].contiguous()  # 丢掉最后一个预测，因为没有target\n",
    "                shift_labels = input_ids[:, 1:].contiguous()   # 丢掉第一个token，因为没有前文可以预测\n",
    "                loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss = loss / grad_accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 反向传播 + 梯度累积\n",
    "            if (global_step + 1) % grad_accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #梯度裁剪，防止梯度爆炸\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            if global_step % 50 == 0:\n",
    "                print(f\"step {global_step:6d} epoch {epoch:3d} loss {loss.item()*grad_accum_steps:.6f}\")\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step >= total_steps:\n",
    "                break\n",
    "\n",
    "            if global_step % save_every == 0:\n",
    "                save_checkpoint(model, optimizer, global_step, path=f'ckpt_step_{global_step}.pt')\n",
    "\n",
    "        print(f'Epoch {epoch} took {time.time()-t0:.2f}s')\n",
    "        if global_step >= total_steps:\n",
    "            break\n",
    "\n",
    "    save_checkpoint(model, optimizer, global_step, path='model/ckpt_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec88d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数配置\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "epochs = 15\n",
    "batch_size = 8\n",
    "seq_len = 128\n",
    "d_model = 256\n",
    "n_layer = 6\n",
    "n_head = 8\n",
    "d_ff = 1024\n",
    "lr = 2e-4\n",
    "total_steps = 1000\n",
    "warmup_steps = 100\n",
    "grad_accum_steps = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e73ad12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 34 Dataset size: 300\n"
     ]
    }
   ],
   "source": [
    "# 数据准备\n",
    "texts = [\n",
    "    \"Hello world! This is a tiny dataset to demonstrate training a small transformer.\",\n",
    "    \"Transformers use attention to mix context. Rotatory embeddings help for autoregressive models.\",\n",
    "    \"This dataset is tiny; in real training you need GBs to TBs of quality data.\"\n",
    "] * 100  # toy dataset 复制放大\n",
    "\n",
    "tokenizer = CharTokenizer(texts)\n",
    "dataset = SimpleTextDataset(texts, tokenizer, seq_len=seq_len)\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size, \"Dataset size:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b57b3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型初始化\n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=seq_len,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.0,\n",
    "    use_rope=True,\n",
    "    checkpoint=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab9fabe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step      0 epoch   0 loss 185.290329\n",
      "Epoch 0 took 4.25s\n",
      "step     50 epoch   1 loss 15.663038\n",
      "Epoch 1 took 3.43s\n",
      "step    100 epoch   2 loss 2.465168\n",
      "Epoch 2 took 3.40s\n",
      "step    150 epoch   3 loss 0.105163\n",
      "Epoch 3 took 3.37s\n",
      "Epoch 4 took 3.50s\n",
      "step    200 epoch   5 loss 0.016550\n",
      "Epoch 5 took 3.54s\n",
      "step    250 epoch   6 loss 0.011935\n",
      "Epoch 6 took 3.51s\n",
      "step    300 epoch   7 loss 0.010812\n",
      "Epoch 7 took 3.33s\n",
      "Epoch 8 took 3.20s\n",
      "step    350 epoch   9 loss 0.017897\n",
      "Epoch 9 took 3.26s\n",
      "step    400 epoch  10 loss 0.018343\n",
      "Epoch 10 took 3.25s\n",
      "step    450 epoch  11 loss 0.008710\n",
      "Epoch 11 took 3.37s\n",
      "Epoch 12 took 3.22s\n",
      "step    500 epoch  13 loss 0.000295\n",
      "Epoch 13 took 3.50s\n",
      "step    550 epoch  14 loss 0.020455\n",
      "Epoch 14 took 3.25s\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "train(\n",
    "    model, dataset, device=device, epochs=epochs,\n",
    "    batch_size=batch_size, seq_len=seq_len,\n",
    "    lr=lr, warmup_steps=warmup_steps, total_steps=total_steps,\n",
    "    grad_accum_steps=grad_accum_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34132865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloworld! This is a tiny dataset to demo tratratratraing a smalll tr\n"
     ]
    }
   ],
   "source": [
    "# 推理示例（采样）\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    ids = torch.tensor([tokenizer(prompt)], dtype=torch.long).to(device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(ids)[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        ids = torch.cat([ids, next_id], dim=1)\n",
    "    return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "print(generate(model, tokenizer, \"Hellow\", max_new_tokens=64, top_k=20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
