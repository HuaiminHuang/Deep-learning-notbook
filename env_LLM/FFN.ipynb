{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43f0028",
   "metadata": {},
   "source": [
    "# FFN 以及改进激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562cba1",
   "metadata": {},
   "source": [
    "FFN（FeedForwardNet）前馈神经网络一般是认为用于存储知识，标准的FFN一般由升维部分和降维部分组成。  \n",
    "$$ \\mathrm{FFN} (x)= RELU(xW_1 + b_1)W_2 +b2$$\n",
    "其中的x的shape为 (batch_size, seq_len, hidden_dim), W_1的shape（h, 4h）,W_2的shape（4h, h）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f1b99",
   "metadata": {},
   "source": [
    "可以从看到$W_1$体现了升维的过程，$W_2$则是降维过程。激活函数体现了输入输出的复杂线性关系，可以替换为其他的激活函数，例如现在常用的GELU（Gaussian Error Linear Unit，GELU）以及swichGELU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1937bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, n_embd=512, dropout=0.1):\n",
    "        self.n_embd = n_embd  # 嵌入维度\n",
    "        self.dropout = dropout  # Dropout比率\n",
    "\n",
    "\n",
    "# FFN实现--(实际上就是一个MLP)\n",
    "class FeedForwardNet(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                  nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "                  nn.Dropout(config.dropout)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd0a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 shape : torch.Size([3, 8, 64])\n",
      "输出 shape : torch.Size([3, 8, 64])\n",
      "参数量   : 33088\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "config = Config(n_embd=64, dropout=0.2)\n",
    "ffn = FeedForwardNet(config)\n",
    "\n",
    "x = torch.randn(3, 8, config.n_embd)\n",
    "with torch.no_grad():\n",
    "    y = ffn(x)\n",
    "\n",
    "print(\"输入 shape :\", x.shape)\n",
    "print(\"输出 shape :\", y.shape)\n",
    "print(\"参数量   :\", sum(p.numel() for p in ffn.parameters())) \n",
    "# (64,)-->(64,64*4)+bias(64*4,) 16640\n",
    "# (64,64*4)-->(64,)+bias(64,) 16448"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e152fa",
   "metadata": {},
   "source": [
    "## ReLU的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c49c6",
   "metadata": {},
   "source": [
    "### 1.ReLU\n",
    "ReLU的公式很简单：\n",
    "$$ ReLU (x)= \\max{(0, x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ab1fa",
   "metadata": {},
   "source": [
    "### 2.GELU \n",
    "从GPT以及BERT以来GRELU使用越来越多：  \n",
    "$$ GELU (x) = xP(X \\le x) = x \\Phi(x) $$\n",
    "其中$\\Phi(x)$是标准正态分布的累计函数：\n",
    "$$\\Phi(x) = \\frac{1}{2}(1 + erf(\\frac{x}{\\sqrt{x}}))$$\n",
    "并且这里的erf是误差函数：\n",
    "$$erf(x) = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}{e^{-t^2}dt}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3dd1d",
   "metadata": {},
   "source": [
    "### 3.SwiGLU\n",
    "SwiGLU（或称swishGLU）是swish激活函数和GLU门控单元的结合。  \n",
    "FFN层把偏置量写入权重里就变成：\n",
    "$$FFN(x) = \\mathrm{ActiveFuntion}(xW_1)W_2$$\n",
    "* swish激活函数：\n",
    "$$\\mathrm{Swish}(x) = x\\sigma(\\beta x) $$\n",
    "其中的$\\beta$是一个超参数，当$\\beta = 1$时就变成SiLU()。  \n",
    "因此采用swish激活函数的FFN:\n",
    "$$FFN (W_1, W_2, x) = \\mathrm{Swish}(xW_1)W_2$$\n",
    "其中的超参数为$W_1, W_2$\n",
    "* GLU门控单元\n",
    "GLU，Gated Linear Units是一种门控结构（有参数，因此相对于普通的激活函数多了一个 gate 矩阵），通过 sigmoid 控制不同维度的激活。公式如下：\n",
    "$$GLU(W, x, V, b, c) = (Wx + b) \\otimes sigmoid(Vx + c)$$\n",
    "而我们都知道 FFN 是一个升高维度，然后降低维度的过程，因此可以写成，W2 是一个降低维度的参数，W1 是升高维度的过程，而 W3 是一个 Gate 需要用到的参数矩阵。\n",
    "$$FFN (w_{up},w_{down},w_{gate})= w_{down} \\times (w_{uo}x \\otimes \\mathrm{Swish}(w_{gate}x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5332f",
   "metadata": {},
   "source": [
    "假设输入的 hidden_dim 大小是 hidden_dim，那么中间层（up 后的维度）大小是 mid_dim， 具体计算逻辑如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374a7d7",
   "metadata": {},
   "source": [
    "mid_dim = int(8 * hidden_dim / 3)  \n",
    "#multiple_of：make SwiGLU hidden layer size multiple of large power of 2  \n",
    "mid_dim = multiple_of * ((mid_dim + multiple_of - 1) // multiple_of)  \n",
    "\n",
    "#multiple_of 一般设置为 256， LLaMA 和 GPT等模型用来优化计算效率  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558fec8",
   "metadata": {},
   "source": [
    "# 3.带有swishGLU的FFN实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02446144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNExper(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = hidden_dim\n",
    "        mid_dim = hidden_dim * 8 // 3\n",
    "\n",
    "        self.up = nn.Linear(hidden_dim, mid_dim, bias=False)\n",
    "        self.down = nn.Linear(mid_dim, hidden_dim, bias=False)\n",
    "        self.gate = nn.Linear(hidden_dim, mid_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dropout(\n",
    "            self.down(\n",
    "                F.silu(self.gate(x))* self.up(x)\n",
    "            )\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722b6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_shape torch.Size([2, 7, 512])\n",
      "our_shpe torch.Size([2, 7, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "torch.manual_seed(42)\n",
    "batch, seq_len, hidden = 2, 7, 512\n",
    "x = torch.randn(batch, seq_len, hidden)\n",
    "\n",
    "# 实例化模型\n",
    "ffn = FFNExper(512, dropout=0.1)\n",
    "ffn.eval()\n",
    "\n",
    "# 前向一次\n",
    "with torch.no_grad():\n",
    "    y = ffn(x)\n",
    "\n",
    "print(\"in_shape\", x.shape)\n",
    "print(\"our_shpe\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247f0cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate(x) 前10个值: \n",
      " tensor([-0.1939, -0.1362,  0.4813, -0.0365,  0.4199,  0.2744, -0.1579, -0.0501,\n",
      "        -0.2211,  0.7552], grad_fn=<SliceBackward0>)\n",
      "up(x)   前10个值: \n",
      " tensor([-7.3304e-01, -9.3535e-04,  1.2379e+00, -4.0043e-01, -6.3098e-01,\n",
      "         1.8584e-01,  6.9051e-01,  9.9118e-01,  4.9408e-01,  1.3586e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "silu(gate) * up 前10个值: \n",
      " tensor([ 1.4213e-01,  1.2744e-04,  5.9577e-01,  1.4631e-02, -2.6498e-01,\n",
      "         5.1000e-02, -1.0906e-01, -4.9707e-02, -1.0923e-01,  1.0260e+00],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mid_dim =  hidden* 8 // 3\n",
    "gate_out = F.silu(ffn.gate(x))  # [B, T, mid_dim]\n",
    "up_out   = ffn.up(x)            # [B, T, mid_dim]\n",
    "print(\"gate(x) 前10个值:\", \"\\n\", gate_out[0, 0, :10])\n",
    "print(\"up(x)   前10个值:\", \"\\n\", up_out[0, 0, :10])\n",
    "print(\"silu(gate) * up 前10个值:\", \"\\n\", (gate_out * up_out)[0, 0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adef25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
