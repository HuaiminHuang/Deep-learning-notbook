{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbbb54-7ef6-4e36-a559-22e059fe0ffd",
   "metadata": {},
   "source": [
    "# Chapter1 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd2d6a2-df1f-4c17-a240-95f8581ced3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 17 11:40:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.88                 Driver Version: 580.88         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P5              4W /  140W |    6073MiB /   8188MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1256    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A            2816      C   ...onda3\\envs\\env_LLM\\python.exe      N/A      |\n",
      "|    0   N/A  N/A            6780    C+G   ...yb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A            8764    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           10232    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12336    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15472    C+G   ...0.3405.102\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e33d6b-66db-402d-87d3-bc92717cd63b",
   "metadata": {},
   "source": [
    "## Qwen/Qwen2.5-0.5B-instruct\n",
    "加载一个模型推理，可以对模型和tokenizer进行分别加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c83e9-43b8-4944-a566-b70225e3f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf93831-bc34-4868-90cb-9f3e3fa1b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c065c2-209d-4947-9403-ddf33b3459ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 8.54s\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Qwen/Qwen2.5-0.5B-instruct',\n",
    "    device_map='cuda',\n",
    "    torch_dtype='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(f\"Model loaded in {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824b7feb-1928-4ced-803e-b26977705581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3250ffa-841a-48c0-88df-6eb964566b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看里面的specali token\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B-instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c2eda5-6bbb-4410-8460-ca81ce3e7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2Tokenizer\n",
    "tk_exp = Qwen2Tokenizer.from_pretrained('Qwen/Qwen2.5-0.5B-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dff7f0e-e67f-493a-8f75-2be601d1280c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91908a7-1d14-4ede-b4ad-b21a61a03f90",
   "metadata": {},
   "source": [
    "# Chapter2 基础使用方式\n",
    "加载好模型之后可以只用原始的model和tokenizer进行推理；\n",
    "\n",
    "* step1：加载模型\n",
    "* step2：构建prompt和tokenizer\n",
    "* step3：推理和解码\n",
    "\n",
    "模型已经加载好了，我们直接从第二部开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9f28bd-d4b3-4327-80c1-4b1bc99c1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第一种手动构建\n",
    "prompt = '请说一个和找工作相关的冷笑话'\n",
    "message = [\n",
    "    {'role': 'system', 'content': 'You are Qwen, Created by Alibaba Clod. You are a helpful assistant.'}, \n",
    "    {'role': 'user', 'content': prompt}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d35c8c-66ea-4b42-9ce7-29962ec0509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    message, \n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63cdddd-5312-44d8-9834-d0c20f7890b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, Created by Alibaba Clod. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "请说一个和找工作相关的冷笑话<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a85954e8-cb66-4ca0-9734-ffec121ad0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer([text], return_tensors='pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7500f166-2309-4fe8-8182-6a85703220e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, Created by Alibaba Clod. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "请说一个和找工作相关的冷笑话<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "========================================\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   4290,\n",
      "            553,  54364,   2435,    347,     13,   1446,    525,    264,  10950,\n",
      "          17847,     13, 151645,    198, 151644,    872,    198,  14880,  36587,\n",
      "          46944,  33108, 114953, 105470,  99476, 109959, 151645,    198, 151644,\n",
      "          77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print('===='*10)\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33976f69-2ab3-45e3-879b-a6625183d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:89: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在一家公司的面试官面前，你突然发现你的简历里没有提到你的家乡。这时，面试官问道：“你在哪里出生的？”你会回答：（笑）“我在美国出生。”\n"
     ]
    }
   ],
   "source": [
    "# 推理和解码\n",
    "generated_ids = model.generate(\n",
    "    **model_input, \n",
    "    max_new_tokens = 512\n",
    ")\n",
    "\n",
    "generated_ids =[\n",
    "    output_ids [len(input_ids):] for input_ids, output_ids in zip(model_input.input_ids, generated_ids)\n",
    "] \n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2baa6a-00be-4a06-a99e-3fd69d3fee2e",
   "metadata": {},
   "source": [
    "使用 transformers的pipeline简化流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a975467a-2f12-43cb-99bc-819e700e2f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# step1 生成pipline\n",
    "generator = pipeline(\n",
    "    'text-generation',  # decoder only   #如果是encoder +decoder使用 text2text-generation\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens = 512, \n",
    "    do_sample = False\n",
    ")\n",
    "\n",
    "# step1 构建一个prompt\n",
    "messages = [\n",
    "    {'role':'user', 'content':'写一个幽默的冷笑话'}\n",
    "]\n",
    "\n",
    "# step3 输出并解码\n",
    "output = generator(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fce8ab-ba7a-4c68-9f9b-ff80acc759d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么程序员总是喜欢用“Hello World”来结束他们的程序？因为这是他们用来庆祝自己成功编写的第一个程序。\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
