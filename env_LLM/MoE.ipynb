{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e888b47",
   "metadata": {},
   "source": [
    "# LLM的MoE-混合专家不同版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2c948",
   "metadata": {},
   "source": [
    "## MoE（Mixture of Experts）设计想解决什么？  \n",
    "1. 模型太大，容量越大效果越好，但是训练推理的成本线性增长。\n",
    "2. MoE 把“一个大网络”拆成“多个小网络（专家）”，每次只激活其中 极少 几个专家，于是：\n",
    " * 参数总量继续膨胀 → 表达能力↑\n",
    " * 计算量几乎不变（只激活 k 个专家）→ 成本≈常数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94130c0",
   "metadata": {},
   "source": [
    "## 核心模块：\n",
    "### 1. 专家网络（Expert Network）\n",
    "\n",
    "通常是普通 FFN（两个线性层 + 非线性），也可以是更复杂的子模型。\n",
    "\n",
    "### 2. 路由/门控函数（Router / Gate）\n",
    "\n",
    "决定每条输入 token 该去哪些专家。\n",
    "数学形式：\n",
    "$$ g(x) = softmax(W_r \\times x)  \\in \\mathbb{R}^E$$\n",
    "其中 $E$ 是专家数，$W_r$是路由权重矩阵。\n",
    "\n",
    "### 3. Top-k 路由策略\n",
    "\n",
    "Switch-Transformer 用 k=1（Top-1 路由）\n",
    "GShard 用 k=2（Top-2 路由）\n",
    "只保留最大的 k 个概率，其余置零 $\\rightarrow$ 稀疏。\n",
    "\n",
    "### 4. 负载均衡损失（Load-Balancing Loss）\n",
    "\n",
    "防止所有 token 都涌向少数“学霸”专家。\n",
    "经典实现：\n",
    "$$ L_{aux} = \\alpha \\times \\sum_{i=1}^E {f_i \\times P_i}$$\n",
    "$f_i$ ：当前 batch 里分配给专家 i 的 token 比例\n",
    "$P_i$：路由给专家 i 的平均门控概率\n",
    "让 $f_i$ ≈ 均匀分布，$P_i$ 也相应均匀，于是 $L_{aux}$ 最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00845ef6",
   "metadata": {},
   "source": [
    "# 1. 基础版本MoE\n",
    "![basic MOE](./img/basic-moe-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d087690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aeaaa",
   "metadata": {},
   "source": [
    "这里简单使用一个Linear层来作为这个专家网络，实际上需要换成FFN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295594c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicExpert(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_in, feature_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74ff475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMOE(nn.Module):\n",
    "    def __init__(self, feature_in, feature_out, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(feature_in, num_experts)\n",
    "        # output shape is (b, num_experts)\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(feature_in, feature_out) for _ in range(num_experts)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape is (b, f_in)  feature_in 也可叫做hidden_size\n",
    "        expert_weights = self.gate(x)\n",
    "        expert_out_list = [expert(x) for expert in self.experts]\n",
    "        # 每个expert输出一个（b, f_out）\n",
    "\n",
    "        #expert_out unsqueeze to (b, 1, feature_out)\n",
    "        expert_outputs= [expert_out.unsqueeze(1) for expert_out in expert_out_list]\n",
    "\n",
    "\n",
    "        expert_output = torch.concat(expert_outputs, dim=1,)\n",
    "        #expert_output shape is (b, num_experts, feature_out)\n",
    "\n",
    "        expert_weights = F.softmax(expert_weights, dim=1) # shape is (b, num_expert)\n",
    "\n",
    "        # 我们希望输出为（batch_size, feature_out）\n",
    "        expert_weights = expert_weights.unsqueeze(1)  # (b, 1, num_expert)\n",
    "        output = expert_weights @ expert_output\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a962e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "def test_BasicMOE():\n",
    "    x = torch.rand(4, 512)\n",
    "    basic_moe = BasicMOE(512, 128, 4)\n",
    "    output = basic_moe(x)\n",
    "    print(output.shape)\n",
    "\n",
    "test_BasicMOE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6efd4",
   "metadata": {},
   "source": [
    "# 2.SparseMoE(现代大模型)\n",
    "和basic的区别是，MoE选择topK个专家，并根据输出进行加权求和，并把输入样本变成了大模型的真实输入Shape，（batch_size, seq_len, hidden_dim）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ab873",
   "metadata": {},
   "source": [
    "### topk()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69520dee",
   "metadata": {},
   "source": [
    "* 选出对应维度（默认dim=-1）的topK（前K最大值），返回值和索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d531ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5430,  0.7799,  1.8441, -0.3242, -0.7400, -1.2461, -0.0387,  1.0213,\n",
      "         1.3348, -0.4689, -0.0103]) \n",
      " tensor([1.8441, 1.5430]) tensor([2, 0])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(11)\n",
    "top2_value, indices = a.topk(2, dim=-1)\n",
    "print(a, \"\\n\", top2_value, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3911cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: torch.Size([3, 3, 5])\n",
      "a:\n",
      " tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.,  9.],\n",
      "         [10., 11., 12., 13., 14.]],\n",
      "\n",
      "        [[15., 16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29.]],\n",
      "\n",
      "        [[30., 31., 32., 33., 34.],\n",
      "         [35., 36., 37., 38., 39.],\n",
      "         [40., 41., 42., 43., 44.]]])\n",
      "values shape: torch.Size([2, 3, 5])\n",
      "indices shape: torch.Size([2, 3, 5])\n",
      "values:\n",
      " tensor([[[30., 31., 32., 33., 34.],\n",
      "         [35., 36., 37., 38., 39.],\n",
      "         [40., 41., 42., 43., 44.]],\n",
      "\n",
      "        [[15., 16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23., 24.],\n",
      "         [25., 26., 27., 28., 29.]]])\n",
      "indices:\n",
      " tensor([[[2, 2, 2, 2, 2],\n",
      "         [2, 2, 2, 2, 2],\n",
      "         [2, 2, 2, 2, 2]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(45).reshape(3, 3, 5).float()\n",
    "print('a shape:', a.shape)        # torch.Size([2, 2, 5])\n",
    "print('a:\\n', a)\n",
    "\n",
    "values, indices = a.topk(2, dim=0)   # k=2，dim=0\n",
    "print('values shape:', values.shape) # torch.Size([2, 2, 5])\n",
    "print('indices shape:', indices.shape) # torch.Size([2, 2, 5])\n",
    "\n",
    "print('values:\\n', values)\n",
    "print('indices:\\n', indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21143b",
   "metadata": {},
   "source": [
    "### 2).one_hot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008ff4e",
   "metadata": {},
   "source": [
    "* torch.nn.functional.one_hot(tensor, num_classes=-1) -> Tensor\n",
    "* 参数：  \n",
    " tensor：任意形状的 整型 张量，元素取值范围 [0, num_classes-1]。num_classes（可选）类别总数。不填（缺省 -1）时，函数内部用 tensor.max()+1 推断  \n",
    "* 返回值：  \n",
    " 新张量比输入 多一维，最后一维长度 = num_classes，\n",
    " 对应位置为 1，其余为 0，dtype 为 torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83924882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 0, 1]])\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([0, 2, 1, 3])   # shape (4,)\n",
    "one_hot = F.one_hot(labels)           # num_classes 自动推断为 4\n",
    "print(one_hot)\n",
    "print(one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4027115",
   "metadata": {},
   "source": [
    "SparseMoE实现代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d2f4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "class MOEConfig:\n",
    "    def __init__(self, \n",
    "                 hidden_dim, expert_number, \n",
    "                 top_k, shared_expert_number = 2\n",
    "                 ):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_number = expert_number\n",
    "        self.top_k = top_k\n",
    "        self.shared_expert_number = shared_expert_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3789b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如何为每个token选专家\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_number)\n",
    "        # 但是后面只选择K个专家\n",
    "\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 假设expert_num =8， top_k = 2\n",
    "        router_logits = self.gate(x)\n",
    "\n",
    "        #计算每一个专家的概率\n",
    "        router_probs = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "\n",
    "        # 计算topk专家的输出\n",
    "        # topk是可以反向传播的\n",
    "        router_weights, selected_expert_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim=-1,\n",
    "        )# router_weights, selected_expert_indices shape are (b * s, top_k)\n",
    "\n",
    "        # 重新归一化\n",
    "        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)\n",
    "        router_weights = router_weights.to(x.dtype) # 防止类型改变\n",
    "\n",
    "        expert_mask = F.one_hot(\n",
    "            selected_expert_indices, \n",
    "            num_classes=self.expert_number,\n",
    "        )# (b * s, top_k, expert_number)\n",
    "\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "        # shape is (expert_number, top_k, b * s)\n",
    "\n",
    "        return router_logits, router_weights, selected_expert_indices, expert_mask\n",
    "        # router_logits (b*s, e_num), \n",
    "        # router_weights (b*s, top_k), \n",
    "        # selected_expert_indices (b*s, top_k), \n",
    "        # expert_mask (expert_num, t, b*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2d7c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主模型\n",
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.top_k = config.top_k\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_number = config.expert_number\n",
    "\n",
    "        # 初始化专家\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(config.hidden_dim, config.hidden_dim, \n",
    "                        )for _ in range(config.expert_number)\n",
    "                    )\n",
    "\n",
    "        self.router = MOERouter(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is (batch_size, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        # token 维度计算 x.reshape(batch * seq_len, hidden_dim)\n",
    "        hidden_states = x.view(-1, hidden_dim)\n",
    "        \n",
    "        # 做相关的专家计算\n",
    "        router_logits, router_weights, _, expert_masks = self.router(\n",
    "            hidden_states\n",
    "            )\n",
    "        \n",
    "        # 需要最后的输出 (batch * seq_len, hidden_dim)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "             dtype=hidden_states.dtype,\n",
    "             device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # 遍历每一个专家\n",
    "        # 把选中的专家的token的hidden_states 加到 final_hidden_states\n",
    "        for expert_idx in range(self.expert_number):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            # expert_masks shape is (expert_num, top_k, b*s )\n",
    "            # current_masks shape is (top_k, b*s )\n",
    "            current_expert_mask = expert_masks[expert_idx]\n",
    "\n",
    "            router_weight_idx, top_x = torch.where(current_expert_mask)\n",
    "            # router_weight_idx 是 0 or 1 (topk=2)\n",
    "            # 表示的是这个token是作为当前专家的 top1 还是 top2\n",
    "            # top_x 的值是 token 在 batch*seq_len 中的位置索引\n",
    "            # 例如对于 batch_size=2, seq_len=4 的输入:\n",
    "            # top_x in [0, 7] 表示在展平后的 8 个 token 的位置\n",
    "            # 作用： router_weight_idx --> weight, top_x --> hidden_states\n",
    "            \n",
    "            # hidden_states (b*s, hidden_dim) --> (1, b*s, hidden_dim) -->\n",
    "            # --> (selected_token_number, hidden_dim)\n",
    "            current_states = hidden_states.unsqueeze(\n",
    "                0)[:, top_x, :].reshape(-1, hidden_dim)\n",
    "            \n",
    "            current_states = expert_layer(current_states)\n",
    "\n",
    "            current_token_router_weight = router_weights[top_x, router_weight_idx]\n",
    "            # 最终的shape就变成了(selected_token_number, )\n",
    "            current_token_router_weight = current_token_router_weight.unsqueeze(-1)\n",
    "            # shape就变成了(selected_token_number, 1)\n",
    "            \n",
    "            #广播 (selected_token_number, hidden_dim) *(selected_token_number, 1)\n",
    "            current_hidden_stasts = current_states * current_token_router_weight\n",
    "            \n",
    "            final_hidden_states.index_add_(\n",
    "                0, \n",
    "                top_x,\n",
    "                current_hidden_stasts   \n",
    "            )\n",
    "        # 把 final_hidden_states 还原到原来的 shape\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "        router_logits = router_logits.view(batch_size, seq_len, -1)    \n",
    "        return final_hidden_states, router_logits # shape 是 (b * s, expert_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ebe9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([2, 4, 64])\n",
      "Output shape: torch.Size([2, 4, 64])\n",
      "Router logits shape: torch.Size([2, 4, 8])\n",
      "Output nan?  tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 配置\n",
    "    hidden_dim = 64\n",
    "    expert_num = 8\n",
    "    top_k = 2\n",
    "    config = MOEConfig(hidden_dim, expert_num, top_k)\n",
    "\n",
    "    # 构造模型和输入\n",
    "    moe = SparseMOE(config)\n",
    "    batch, seq_len = 2, 4\n",
    "    x = torch.randn(batch, seq_len, hidden_dim)\n",
    "\n",
    "    # 前向\n",
    "    out, router_logits = moe(x)\n",
    "\n",
    "    print(\"Input shape :\", x.shape)\n",
    "    print(\"Output shape:\", out.shape)            # 应为 (2,4,64)\n",
    "    print(\"Router logits shape:\", router_logits.shape)  # 应为 (2,4,8)\n",
    "    print(\"Output nan? \", torch.isnan(out).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403076d",
   "metadata": {},
   "source": [
    "# 3.share_expert_sparseMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b52dbea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "class ShareExpertMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.moe_model = SparseMOE(config)\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(\n",
    "                    config.hidden_dim, config.hidden_dim\n",
    "                ) for _ in range(config.shared_expert_number)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape 是 (b, s, hidden_dim)\n",
    "        # 首先过 moe 模型\n",
    "        sparse_moe_out, router_logits = self.moe_model(x)\n",
    "        \n",
    "        # 针对的还是 x 的每一个 \n",
    "        # 然后过 shared experts\n",
    "        shared_experts_out = [\n",
    "            expert(x) for expert in self.shared_experts\n",
    "        ] # 每一个 expert 的输出 shape 是 (b, s, hidden_dim)\n",
    "        \n",
    "        shared_experts_out = torch.stack(\n",
    "            shared_experts_out, dim=0\n",
    "        ).sum(dim=0)\n",
    "        \n",
    "        # 把 sparse_moe_out 和 shared_experts_out 加起来\n",
    "        return sparse_moe_out + shared_experts_out, router_logits\n",
    "\n",
    "\n",
    "def test_share_expert_moe():\n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MOEConfig(16, 2, 2)\n",
    "    share_expert_moe = ShareExpertMOE(config)\n",
    "    out = share_expert_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "\n",
    "test_share_expert_moe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cc112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
