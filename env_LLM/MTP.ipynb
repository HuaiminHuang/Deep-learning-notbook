{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0408d19",
   "metadata": {},
   "source": [
    "![MTP](./img/MTP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d757d",
   "metadata": {},
   "source": [
    "## Multi-Token Prediction (MTP) 总体思路\n",
    "\n",
    "### 目标与动机\n",
    "  - **目标**：在每个位置预测多个未来令牌（$t_{i+1}, t_{i+2}, \\dots, t_{i+D}$）\n",
    "  - **动机**：\n",
    "    - 增加训练信号密度，提高数据效率\n",
    "    - 帮助模型预规划表示，改善未来预测\n",
    "    - 保持完整因果链（顺序预测而非并行）\n",
    "\n",
    "### 架构概览\n",
    "  - **模块数量**：$D$个顺序MTP模块（对应$D$个预测深度）\n",
    "  - **共享组件**：嵌入层$\\text{Emb}(\\cdot)$和输出头$\\text{OutHead}(\\cdot)$与主模型共享\n",
    "  - **专用组件**：每个深度$k$有专用Transformer块$\\text{TRM}_k(\\cdot)$和投影矩阵 $M_k$\n",
    "\n",
    "\n",
    "## 公式原理\n",
    "\n",
    "### 1. 表示结合与投影\n",
    "  对于第$i$个令牌，在深度$k$的表示计算：\n",
    "  $$h_i^k = M_k \\left[ \\text{RMSNorm}(h_i^{k-1}); \\text{RMSNorm}(\\text{Emb}(t_{i+k})) \\right]$$\n",
    "  其中：\n",
    "  - $h_i^{k-1} \\in \\mathbb{R}^d$：前一个深度的表示\n",
    "  - $\\text{Emb}(t_{i+k}) \\in \\mathbb{R}^d$：未来令牌的嵌入\n",
    "  - $M_k \\in \\mathbb{R}^{d \\times 2d}$：投影矩阵\n",
    "  - $[:,:]$：向量连接操作\n",
    "\n",
    "**特殊情况**：当$k=1$时，$h_i^{k-1}$来自主模型输出。\n",
    "\n",
    "### 2. Transformer处理\n",
    "  将投影后的表示输入Transformer块：\n",
    "  $$h_{1:T-k}^k = \\text{TRM}_k(h_{1:T-k}^{tk})$$\n",
    "  - $T$：序列长度\n",
    "  - $t_{ij}$：切片操作（包含边界）\n",
    "\n",
    "### 3. 概率分布计算\n",
    "  使用共享输出头预测概率：\n",
    "  $$P_{i+k+1}^k = \\text{OutHead}(h_i^k)$$\n",
    "  - $\\text{OutHead}(\\cdot)$：线性映射 + Softmax\n",
    "  - $P_{i+k+1}^k \\in \\mathbb{R}^V$：词汇表上的概率分布\n",
    "  - $V$：词汇表大小\n",
    "\n",
    "## Loss计算\n",
    "\n",
    "### 单个深度损失\n",
    "  对于深度$k$的交叉熵损失：\n",
    "  $$\\mathcal{L}_{MTP}^{k} = -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_i^k[t_i]$$\n",
    "  其中：\n",
    "  - $t_i$：第$i$个位置的真实令牌\n",
    "  - $P_i^k[t_i]$：模型对$t_i$的预测概率\n",
    "\n",
    "### 整体MTP损失\n",
    "  对所有深度取平均并加权：\n",
    "  $$\\mathcal{L}_{MTP} = \\frac{\\lambda}{D} \\sum_{k=1}^{D} \\mathcal{L}_{MTP}^k$$\n",
    "  - $\\lambda$：权重因子（超参数）\n",
    "  - $D$：总预测深度\n",
    "\n",
    "## 推理阶段\n",
    "\n",
    "### 主要模式\n",
    "  $$\\text{推理} = \\text{主模型（丢弃MTP模块）}$$\n",
    "  - MTP模块仅用于训练增强\n",
    "  - 推理时主模型独立工作，无需修改\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f62407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3059ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取model对应的 transformers block\n",
    "model_path = \"../model/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afa408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[transformers.cache_utils.Cache] = None, use_cache: Optional[bool] = False, cache_position: Optional[torch.LongTensor] = None, position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None, **kwargs: typing_extensions.Unpack[transformers.utils.generic.TransformersKwargs]) -> torch.Tensor>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
    "import inspect\n",
    "\n",
    "inspect.signature(Qwen2DecoderLayer.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d0869",
   "metadata": {},
   "source": [
    "MTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModule(nn.Module):\n",
    "    \"\"\"轻量 MLP 版本（作为 Transformer block 的 fallback）\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.ff1 = nn.Linear(2 * hidden_size, 4 * hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.ff2 = nn.Linear(4 * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, S, 2H)\n",
    "        x = self.ff1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.ff2(x)\n",
    "        return x  # (B, S, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b12665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    从 main_model 中拿到一个 Transformer block\n",
    "    对于 concat 后的 proj_down : 2H --> H\n",
    "    \"\"\"\n",
    "    def __init__(self, block_module, hidden_size):\n",
    "        super().__init__()\n",
    "        # block_module 是从主模型复制来的单层 transformer block（nn.Module）\n",
    "        self.block = copy.deepcopy(block_module)\n",
    "        # 如果 block 的输入期望 H 维，但我们拼接后是 2H，先用线性降维\n",
    "        self.input_proj = nn.Linear(2 * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, prev_hidden, input_embed, attention_mask=None, **kwargs):\n",
    "        # prev_hidden & input_embed: (B, S, H)\n",
    "        x = torch.cat([prev_hidden, input_embed], dim=-1)  # (B,S,2H)\n",
    "        x = self.input_proj(x)  # (B,S,H)\n",
    "        # 假定 block 的 forward 接口为 block(x, attention_mask=...)\n",
    "        out = self.block(x, attention_mask=attention_mask, **kwargs)\n",
    "        # block 可能返回 tuple，取第0项作为 hidden\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            out = out[0]\n",
    "        return out  # (B,S,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48be5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTPHead(nn.Module):\n",
    "    \"\"\"把 hidden -> vocab logits 的 head，便于做权重共享或替换\"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size, tie_embedding=None):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # 权重绑定到 embedding 的 weight（weight tying）\n",
    "        if tie_embedding is not None:\n",
    "            self.linear.weight = tie_embedding\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logits = self.linear(hidden_states)  # (B, S, H) --> (B, S, V)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b5c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 predict_tokens_num: int = 5,\n",
    "                 mtp_lambda: float = 0.5,\n",
    "                 random_depth_rate: float = 1.0,  # 1.0 表示每个 batch 全部 depth 都计算；<1 表示随机采样部分 depth\n",
    "                 use_mlp: bool = True,\n",
    "                 freeze_base_model: bool = True,\n",
    "                 use_peft: bool = False\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        random_depth_rate:\n",
    "          - 1.0: 训练时计算所有 MTP depth 的 loss\n",
    "          - 0.5: 随机采样约一半的 depth 来计算（节省计算）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.predict_tokens_num = predict_tokens_num\n",
    "        self.mtp_lambda = mtp_lambda\n",
    "        self.random_depth_rate = random_depth_rate\n",
    "        self.use_mlp = use_mlp\n",
    "        self.use_peft = use_peft    \n",
    "\n",
    "        # 载入主模型（取 base_model 方便直接拿 last_hidden_state / embeddings）\n",
    "        if self.use_peft:\n",
    "            self.main_model = model\n",
    "        else:\n",
    "            self.main_model = model.base_model\n",
    "\n",
    "\n",
    "        # 冻结基础模型参数（关键步骤！）\n",
    "        if freeze_base_model:\n",
    "            for param in self.main_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            if hasattr(model, 'lm_head'):\n",
    "                for param in model.lm_head.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        H = self.main_model.config.hidden_size\n",
    "        V = self.main_model.config.vocab_size\n",
    "\n",
    "        mtp_modules = []\n",
    "\n",
    "        if use_mlp:\n",
    "            for _ in range(self.predict_tokens_num-1):\n",
    "                mtp_modules.append(MLPModule(H))\n",
    "        else:\n",
    "            transformers_block = self.main_model.layers[0]\n",
    "            for _ in range(self.predict_tokens_num-1):\n",
    "                mtp_modules.append(\n",
    "                    TransformerWrapper(transformers_block, H)\n",
    "                )\n",
    "        self.mtp_modules = nn.ModuleList(mtp_modules)\n",
    "\n",
    "        # 输出 head，默认与 embedding weight tying\n",
    "        embedding_weight = self.main_model.get_input_embeddings().weight\n",
    "        self.output_head = MTPHead(H, V, tie_embedding=embedding_weight)\n",
    "\n",
    "    def forward_main(self, input_ids, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        main_hidden_output: (B,S,H) and main_logits: (B,S,V)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self.use_peft:\n",
    "                base = self.main_model.get_base_model()          \n",
    "                outputs = base(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=True,           # 显式要求返回 hidden_states\n",
    "                            **kwargs)\n",
    "                last_hidden = outputs.hidden_states[-1] \n",
    "            else:    \n",
    "                outputs = self.main_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "                last_hidden = outputs.last_hidden_state  # (B,S,H)\n",
    "            logits = self.output_head(last_hidden)   # (B,S,V)          # 取最后一层\n",
    "        return last_hidden, logits\n",
    "\n",
    "    def forward_mtp_once(self, input_ids, prev_hidden, head_index, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        mtp_hidden: (B,S,H) and mtp_logits: (B,S,V)\n",
    "        如果使用 transformer wrapper，需要传 attention_mask\n",
    "        \"\"\"\n",
    "        # embedding\n",
    "        input_embed = self.main_model.get_input_embeddings()(input_ids)  # (B,S,H)\n",
    "        if self.use_mlp:\n",
    "            # 直接拼接并输入MLP\n",
    "            concat = torch.cat([prev_hidden, input_embed], dim=-1)  # (B,S,2H)\n",
    "            mtp_hidden = self.mtp_modules[head_index](concat)  # (B,S,H)\n",
    "        else:\n",
    "            # Transformer版本,使用TransformerWrapper\n",
    "            module = self.mtp_modules[head_index]\n",
    "            mtp_hidden = module(prev_hidden, input_embed, \n",
    "                                attention_mask=attention_mask, **kwargs)\n",
    "        mtp_logits = self.output_head(mtp_hidden)  # (B,S,V)\n",
    "        return mtp_hidden, mtp_logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, training=True, sample_depths=None, **kwargs):\n",
    "        \"\"\"\n",
    "        若 training=True：默认返回 dict 包含 'head_main' 和 'mtp_head_i'（可能只包含部分 depth，受 sample_depths 控制）\n",
    "        若 training=False：只返回 main head 的 logits（推理）\n",
    "        sample_depths: 如果为 None，依据 random_depth_rate 决定是否随机采样 depths 以节省计算\n",
    "        \"\"\"\n",
    "        outputs = {}\n",
    "        main_hidden, main_logits = self.forward_main(input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        outputs['head_main'] = main_logits\n",
    "        if not training:\n",
    "            return outputs  # 推理只需主 head\n",
    "\n",
    "        # 训练时决定计算哪些 depth\n",
    "        D = self.predict_tokens_num - 1\n",
    "        if sample_depths is None:\n",
    "            if self.random_depth_rate >= 1.0:  # deafult\n",
    "                sample_depths = list(range(D))\n",
    "            else:\n",
    "                # 随机采样 depth 索引 随机选择 num 个\n",
    "                num = max(1, int(D * self.random_depth_rate))\n",
    "                all_idx = list(range(D))\n",
    "                sample_depths = sorted(torch.randperm(D)[:num].tolist())\n",
    "                \n",
    "        prev_hidden = main_hidden\n",
    "        for idx in range(D):\n",
    "            # 如果当前 depth 不在要计算的 sample_depths，仍需推进 prev_hidden（因为 MTP 是链式）\n",
    "            prev_hidden, mtp_logits = self.forward_mtp_once(input_ids, prev_hidden, idx, attention_mask=attention_mask, **kwargs)\n",
    "            # 只有在 sample_depths 中才把 logits 写入 outputs（避免不必要的内存）\n",
    "            if idx in sample_depths:\n",
    "                outputs[f'mtp_head_{idx}'] = mtp_logits\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_loss(self, outputs: dict, labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        统一计算 main loss 与 mtp losses。\n",
    "        labels: (B, S), 且 pad 已经被 -100 填充\n",
    "        返回： main_loss, mtp_loss (可能为 0), total_loss\n",
    "        \"\"\"\n",
    "        device = labels.device\n",
    "        main_logits = outputs['head_main']  # (B,S,V)\n",
    "        B, S, V = main_logits.shape\n",
    "\n",
    "        # main loss: predict t+1\n",
    "        main_logits_flat = main_logits[:, :-1, :].reshape(-1, V)\n",
    "        main_targets = labels[:, 1:].reshape(-1)\n",
    "        main_loss = F.cross_entropy(main_logits_flat, main_targets, ignore_index=-100)\n",
    "\n",
    "        # mtp losses: 对每个存在的 mtp_head_i 进行对齐计算\n",
    "        mtp_losses = []\n",
    "        for key in outputs:\n",
    "            if not key.startswith('mtp_head_'):\n",
    "                continue\n",
    "            idx = int(key.split('_')[-1])  # 0-based head index\n",
    "            mtp_logits = outputs[key]  # (B,S,V)\n",
    "            offset = idx + 2  # head_index=0 -> predict t+2\n",
    "            valid_len = S - offset\n",
    "            if valid_len <= 0:\n",
    "                continue\n",
    "            logits = mtp_logits[:, :valid_len, :].reshape(-1, V)\n",
    "            targets = labels[:, offset:offset+valid_len].reshape(-1)\n",
    "            loss_i = F.cross_entropy(logits, targets, ignore_index=-100)\n",
    "            mtp_losses.append(loss_i)\n",
    "\n",
    "        if len(mtp_losses) > 0:\n",
    "            mtp_loss = torch.stack(mtp_losses).mean()\n",
    "            total_loss = main_loss + self.mtp_lambda * mtp_loss\n",
    "        else:\n",
    "            mtp_loss = torch.tensor(0.0, device=device)\n",
    "            total_loss = main_loss\n",
    "\n",
    "        return main_loss, mtp_loss, total_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_autoregressive(self, input_ids, attention_mask=None, max_length=50):\n",
    "        \"\"\"\n",
    "        简化的生成函数：不启用 speculative decoding，仅用主模型逐步生成。\n",
    "        若需要复杂的 speculative decoding，可以在此基础上扩展。\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = input_ids.device\n",
    "        seq = input_ids.clone()\n",
    "        batch_size = seq.size(0)\n",
    "        for _ in range(max_length - seq.size(1)):\n",
    "            outputs = self.forward(seq, attention_mask=attention_mask, training=False)\n",
    "            logits = outputs['head_main']  # (B, S, V)\n",
    "            next_logits = logits[:, -1, :]  # (B, V)\n",
    "            next_token = torch.argmax(next_logits, dim=-1, keepdim=True)  # (B,1)\n",
    "            seq = torch.cat([seq, next_token], dim=1)\n",
    "        return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7243757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MTP, dataloader, optimizer, writer, device='cuda',\n",
    "          epochs=5, print_step=10, save_step=1000, save_path='../model/mtp/checkpoint',\n",
    "          mtp_lambda=0.5, grad_clip=1.0):\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    scaler = GradScaler()\n",
    "    model.to(device)\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 混合精度前向传播\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=None, training=True)\n",
    "                main_loss, mtp_loss, total_loss = model.compute_loss(outputs, labels)\n",
    "\n",
    "            # 正确的顺序：scale -> backward -> unscale -> clip -> step -> update\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            # 取消缩放梯度\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # 梯度裁剪（必须在unscale之后）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            # 更新参数\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # 记录日志\n",
    "            if steps % print_step == 0:\n",
    "                writer.add_scalar('train/main_loss', main_loss.item(), steps)\n",
    "                writer.add_scalar('train/mtp_loss', mtp_loss.item(), steps)\n",
    "                writer.add_scalar('train/total_loss', total_loss.item(), steps)\n",
    "                print(f\"[Epoch {epoch+1}] Step {steps}, main_loss={main_loss.item():.4f}, mtp_loss={mtp_loss.item():.4f}, total_loss={total_loss.item():.4f}\")\n",
    "\n",
    "            if steps % save_step == 0 and steps > 0:\n",
    "                torch.save({\n",
    "                    'model_state': model.state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict(),\n",
    "                    'step': steps,\n",
    "                    'scaler_state': scaler.state_dict()\n",
    "                }, f\"{save_path}/checkpoint_{steps}.pt\")\n",
    "\n",
    "            steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8afd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        user = sample[\"input\"]\n",
    "        assistant = sample[\"target\"]\n",
    "\n",
    "        # 构造 prompt\n",
    "        q = self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": user}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # 拼接答案（带 eos）\n",
    "        a = assistant + self.tokenizer.eos_token\n",
    "\n",
    "        q_input_ids = self.tokenizer(q)[\"input_ids\"]\n",
    "        a_input_ids = self.tokenizer(a)[\"input_ids\"]\n",
    "\n",
    "        input_ids = q_input_ids + a_input_ids\n",
    "        labels = [-100] * len(q_input_ids) + a_input_ids\n",
    "\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            labels = labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d10b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        max_len = max(len(feature['input_ids']) for feature in features)\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        attention_mask = []\n",
    "\n",
    "        for feature in features:\n",
    "            # pad input_ids\n",
    "            padding_len = max_len - len(feature[\"input_ids\"])\n",
    "            input_ids.append(feature[\"input_ids\"] + [self.tokenizer.pad_token_id] * padding_len)\n",
    "\n",
    "            # pad labels (用 -100 而不是 pad_token_id)\n",
    "            labels.append(feature[\"labels\"] + [-100] * padding_len)\n",
    "\n",
    "            # attention_mask\n",
    "            attention_mask.append([1] * len(feature[\"input_ids\"]) + [0] * padding_len)\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e62518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['kind', 'input', 'target'],\n",
       "    num_rows: 1649399\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"YeungNLP/firefly-train-1.1M\")\n",
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a9d871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds[\"train\"].shuffle(42).select(range(2000))\n",
    "train_data = MyDataset(dataset, tokenizer, max_length=512)\n",
    "collator = MyDataCollator(tokenizer)\n",
    "loader = DataLoader(train_data, batch_size=2, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fbea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "use_peft=False\n",
    "freeze_base_model = True\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "if use_peft:\n",
    "    model = get_peft_model(model, lora_config)     \n",
    "mtp_model = MTP(model=model,\n",
    "                predict_tokens_num=5,\n",
    "                mtp_lambda=0.3,\n",
    "                use_mlp=True,\n",
    "                freeze_base_model=freeze_base_model,\n",
    "                use_peft=use_peft)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2193a9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可训练参数总数: 174.84M\n",
      "总参数: 532.74M\n",
      "训练参数占比: 32.82%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    打印模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"可训练参数总数: {trainable_params / 1e6:.2f}M\")\n",
    "    print(f\"总参数: {all_params / 1e6:.2f}M\")\n",
    "    print(f\"训练参数占比: {100 * trainable_params / all_params:.2f}%\")\n",
    "    return trainable_params, all_params\n",
    "\n",
    "trainable_params, all_params = print_trainable_parameters(mtp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "227ba9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:83: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Step 0, main_loss=4.9228, mtp_loss=12.0181, total_loss=8.5282\n",
      "[Epoch 1] Step 10, main_loss=4.0091, mtp_loss=9.8869, total_loss=6.9752\n",
      "[Epoch 1] Step 20, main_loss=1.8060, mtp_loss=9.1845, total_loss=4.5614\n",
      "[Epoch 1] Step 30, main_loss=2.4778, mtp_loss=8.7691, total_loss=5.1086\n",
      "[Epoch 1] Step 40, main_loss=2.5200, mtp_loss=9.1255, total_loss=5.2576\n",
      "[Epoch 1] Step 50, main_loss=3.0501, mtp_loss=8.8223, total_loss=5.6968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/mtp3/runs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(mtp_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmtp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../model/mtp3/checkpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, writer, device, epochs, print_step, save_step, save_path, mtp_lambda, grad_clip)\u001b[0m\n\u001b[0;32m     21\u001b[0m     main_loss, mtp_loss, total_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(outputs, labels)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 正确的顺序：scale -> backward -> unscale -> clip -> step -> update\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 取消缩放梯度\u001b[39;00m\n\u001b[0;32m     27\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('../model/mtp3/runs')\n",
    "optimizer = torch.optim.Adam(mtp_model.parameters(), lr=1e-4)\n",
    "train(mtp_model, loader, optimizer, \n",
    "      writer, device='cuda', \n",
    "      epochs=1, print_step=10, \n",
    "      save_step=1000, save_path='../model/mtp3/checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2e9c3",
   "metadata": {},
   "source": [
    "![MTP loss](./img/MTP_loss.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
