{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cef566",
   "metadata": {},
   "source": [
    "# multi-head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81c0ba",
   "metadata": {},
   "source": [
    "![MLA](./img/MLA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da288f71",
   "metadata": {},
   "source": [
    "## Preliminaries: Standard Multi-Head Attention\n",
    "- d is embedding dimension \n",
    "- $n_h$ number of attention heads \n",
    "- $d_h$ dimension per head\n",
    "- $h_t \\in \\mathbb{R}^{d} $ be the attention input of the $t$-th token at an attention layer\n",
    "\n",
    "设输入token，标准的 MHA 首先通过三个投影矩阵（proj matrices）：$W^{Q}, W^{K}, W^{V} \\in \\mathbb{R}^{d_h n_h \\times d}$ 生成 $\\mathbf{q_t}, \\mathbf{k_t}, \\mathbf{v_t} \\in \\mathbb{R}^{d_h n_h}$：\n",
    "\n",
    "$$\\mathbf{q_t} = W^{Q}\\mathbf{h_t},$$\n",
    "$$\\mathbf{k_t} = W^{K}\\mathbf{h_t},$$\n",
    "$$\\mathbf{v_t} = W^{V}\\mathbf{h_t},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cb222",
   "metadata": {},
   "source": [
    "Then, $\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t$ will be sliced into $n_h$ heads for the multi-head attention computation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "[\\mathbf{q}_{t,1}; \\mathbf{q}_{t,2}; \\ldots; \\mathbf{q}_{t,n_h}] &= \\mathbf{q}_t, \\\\\n",
    "[\\mathbf{k}_{t,1}; \\mathbf{k}_{t,2}; \\ldots; \\mathbf{k}_{t,n_h}] &= \\mathbf{k}_t, \\\\\n",
    "[\\mathbf{v}_{t,1}; \\mathbf{v}_{t,2}; \\ldots; \\mathbf{v}_{t,n_h}] &= \\mathbf{v}_t,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{o}_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}_j \\left( \\frac{\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h}} \\right) \\mathbf{v}_{j,i},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_t = \\mathbf{W}^O [\\mathbf{o}_{t,1}; \\mathbf{o}_{t,2}; \\ldots; \\mathbf{o}_{t,n_h}],\n",
    "$$\n",
    "- $\\mathbf{q_{t,i}}, \\mathbf{k_{t,i}}, \\mathbf{v_{t,i}} \\in \\mathbb{R}^{d_h}$ 是第i个attention head 。\n",
    "- $W^O \\in \\mathbb{R}^{d \\times d_h n_h}$ 表示输出的投影矩阵\n",
    "- **Total KV-cache**: $2 * n_h * d_h * l$ 对于整个序列的token都需要缓存k和v(l is number of layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86803cbd",
   "metadata": {},
   "source": [
    "## Low-Rank Key-Value Joint Compression\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{c}_t^{KV} &= \\mathbf{W}^{DKV} \\mathbf{h}_t, \\\\\n",
    "\\mathbf{k}_t^C &= \\mathbf{W}^{UK} \\mathbf{c}_t^{KV}, \\\\\n",
    "\\mathbf{v}_t^C &= \\mathbf{W}^{UV} \\mathbf{c}_t^{KV},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $c_t^{KV} \\in \\mathbb{R}^{d_c}$ 是一个 compressed latent 向量 (for k, v)，并且 $d_c (\\ll d_h n_h)$\n",
    "- $\\mathbf{W}^{DKV} \\in \\mathbb{R}^{d_c \\times d}$ 是一个降维的投影矩阵\n",
    "- $\\mathbf{W}^{UK}, \\mathbf{W}^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}$ 是升维投影矩阵\n",
    "- **KV cache** 这样就下降到了：$d_c * l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd9692",
   "metadata": {},
   "source": [
    "RoPE解耦策略：引入解耦的qk携带位置编码\n",
    "\n",
    " - $\\mathbf{q}_{t,i}^R \\in \\mathbb{R}^{d_h^R}$ and a shared key $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$ to carry RoPE, \n",
    " - $d_h^R$ denotes the per-head dimension of the decoupled queries and key.\n",
    "\n",
    "$$\n",
    "[\\mathbf{q}_{t,1}^R; \\mathbf{q}_{t,2}^R; \\ldots; \\mathbf{q}_{t,n_h}^R] = \\mathbf{q}_t^R = \\text{RoPE}(W^{QR} \\mathbf{c}_t^Q), \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}_t^R = \\text{RoPE}(W^{KR} \\mathbf{h}_t), \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{q}_{t,i} = [\\mathbf{q}_{t,i}^C; \\mathbf{q}_{t,i}^R],\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}_{t,i} = [\\mathbf{k}_{t,i}^C; \\mathbf{k}_t^R], \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{o}_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}_j\\left(\\frac{\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h + d_h^R}}\\right) \\mathbf{v}_{j,i}^C \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_t = W^O [\\mathbf{o}_{t,1}; \\mathbf{o}_{t,2}; \\ldots; \\mathbf{o}_{t,n_h}], \n",
    "$$\n",
    "- 注解：\n",
    "    - $W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}$ and $W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}$ 为解耦q, k矩阵\n",
    "    - $\\text{RoPE}(\\cdot)$ 为旋转位置编码算子\n",
    "    - $[\\cdot; \\cdot]$ 为concat算子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e905de",
   "metadata": {},
   "source": [
    "# MLA 整体流程\n",
    "\n",
    "MLA 的流程涉及多个步骤，包括压缩、解耦 RoPE 和注意力计算。假设输入隐藏状态维度为 $d$，注意力头数为 $n_h$，每个头的维度为 $d_h$，KV 压缩维度为 $d_c$（满足 $d_c \\ll d_h n_h$），解耦 RoPE 的维度为 $d_h^R$。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 低秩 KV 压缩\n",
    "- 输入 $h_t \\in \\mathbb{R}^d$，首先生成压缩的潜在向量：\n",
    "  $$\n",
    "  \\mathbf{c}_t^{KV} = W^{DKV}h_t\n",
    "  $$\n",
    "  其中 $W^{DKV} \\in \\mathbb{R}^{d_c \\times d}$，因此 $\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}$。\n",
    "\n",
    "- 然后通过上投影矩阵生成压缩的 Key 和 Value：\n",
    "  $$\n",
    "  \\mathbf{k}_t^C = W^{UK} \\mathbf{c}_t^{KV}, \\quad \\mathbf{v}_t^C = W^{UV} \\mathbf{c}_t^{KV}\n",
    "  $$\n",
    "  其中 $W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}$，因此 $\\mathbf{k}_t^C, \\mathbf{v}_t^C \\in \\mathbb{R}^{d_h n_h}$。  \n",
    "  这些向量通常被重塑为 $n_h \\times d_h$ 用于多头注意力。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 查询压缩（可选，用于对称性）\n",
    "- 查询压缩：\n",
    "  $$\n",
    "  \\mathbf{c}_t^Q = W^{DQ}h_t\n",
    "  $$\n",
    "  其中 $W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}$，$\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}$（通常 $d_c' = d_c$）。\n",
    "\n",
    "- 上投影：\n",
    "  $$\n",
    "  \\mathbf{q}_t^C = W^{UQ} \\mathbf{c}_t^Q\n",
    "  $$\n",
    "  其中 $W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}$，因此 $\\mathbf{q}_t^C \\in \\mathbb{R}^{d_h n_h}$。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 解耦 RoPE\n",
    "为了解决 RoPE 与低秩压缩的兼容性问题，引入解耦的查询和键。\n",
    "\n",
    "- **解耦查询**  \n",
    "  通过矩阵 $W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}$ 生成：\n",
    "  $$\n",
    "  \\mathbf{q}_t^R = \\text{RoPE}(W^{QR} \\mathbf{c}_t^Q)\n",
    "  $$\n",
    "  其中 $\\mathbf{q}_t^R \\in \\mathbb{R}^{d_h^R n_h}$，被重塑为 $n_h \\times d_h^R$。RoPE 应用于每个头的位置。\n",
    "\n",
    "- **解耦键**  \n",
    "  通过矩阵 $W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}$ 生成共享键：\n",
    "  $$\n",
    "  \\mathbf{k}_t^R = \\text{RoPE}(W^{KR}h_t)\n",
    "  $$\n",
    "  其中 $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$，所有头共享同一个解耦键。RoPE 同样应用。\n",
    "\n",
    "- **拼接操作**\n",
    "  - 对于每个头 $i$，查询拼接：  \n",
    "    $\\mathbf{q}_{t,i} = [\\mathbf{q}_{t,i}^C; \\mathbf{q}_{t,i}^R]$  \n",
    "    → 每个头的查询维度变为 $d_h + d_h^R$。\n",
    "  - 对于每个头 $i$，键拼接：  \n",
    "    $\\mathbf{k}_{t,i} = [\\mathbf{k}_{t,i}^C; \\mathbf{k}_{t,i}^R]$  \n",
    "    → 每个头的键维度变为 $d_h + d_h^R$（$\\mathbf{k}_t^R$ 被广播到所有头）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 注意力计算\n",
    "- 注意力得分（每个头 $i$）：\n",
    "  $$\n",
    "  \\text{score} = \\frac{\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h + d_h^R}}\n",
    "  $$\n",
    "\n",
    "- 输出（每个头）：\n",
    "  $$\n",
    "  \\mathbf{o}_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}_j(\\text{score}) \\mathbf{v}_{j,i}^C\n",
    "  $$\n",
    "  其中 $\\mathbf{v}_{j,i}^C$ 是压缩 Value 的第 $i$ 个头，维度 $d_h$。\n",
    "\n",
    "- 拼接所有头的输出并线性投影：\n",
    "  $$\n",
    "  \\mathbf{u}_t = W^O [\\mathbf{o}_{t,1}; \\mathbf{o}_{t,2}; \\ldots; \\mathbf{o}_{t,n_h}]\n",
    "  $$\n",
    "  其中 $W^O \\in \\mathbb{R}^{d \\times d_h n_h}$。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. KV 缓存\n",
    "推理时，需要缓存每层的两个向量：\n",
    "- 压缩潜在向量 $\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}$\n",
    "- 解耦键 $\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}$\n",
    "\n",
    "因此，每个 token 每层的缓存维度为：\n",
    "$$\n",
    "d_c + d_h^R\n",
    "$$\n",
    "\n",
    "对于序列长度 $S$ 和层数 $L$，总缓存大小为：\n",
    "$$\n",
    "S \\times (d_c + d_h^R) \\times L\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f2ed2",
   "metadata": {},
   "source": [
    "# 关于矩阵吸收\n",
    "\n",
    "- Key 吸收：\n",
    "  - 在计算注意力得分时，需要计算压缩查询和压缩键的点积：$(\\mathbf{q}_t^C)^T \\mathbf{k}_j^C$。\n",
    "  - 由于 $\\mathbf{q}_t^C = W^{UQ} \\mathbf{c}_t^Q$ 和 $\\mathbf{k}_j^C = W^{UK} \\mathbf{c}_j^{KV}$，我们有：\n",
    "    $$\n",
    "    (\\mathbf{q}_t^C)^T \\mathbf{k}_j^C = (\\mathbf{c}_t^Q)^T (W^{UQ})^T W^{UK} \\mathbf{c}_j^{KV}\n",
    "    $$\n",
    "  - 预先计算矩阵 $W^{K'} = (W^{UQ})^T W^{UK} \\in \\mathbb{R}^{d_c' \\times d_c}$，则点积可通过低维向量计算：\n",
    "    $$\n",
    "    (\\mathbf{q}_t^C)^T \\mathbf{k}_j^C = (\\mathbf{c}_t^Q)^T W^{K'} \\mathbf{c}_j^{KV}\n",
    "    $$\n",
    "  - 这样无需显式生成高维的 $\\mathbf{q}_t^C$ 和 $\\mathbf{k}_j^C$。\n",
    "\n",
    "- Value 吸收：\n",
    "  - 注意力输出后需要投影。注意力输出是加权和的 $\\mathbf{v}_j^C$，而 $\\mathbf{v}_j^C = W^{UV} \\mathbf{c}_j^{KV}$。\n",
    "  - 输出投影时：\n",
    "    $$\n",
    "    \\mathbf{u}_t = W^O \\left( \\sum_j \\alpha_j \\mathbf{v}_j^C \\right) = W^O \\left( \\sum_j \\alpha_j W^{UV} \\mathbf{c}_j^{KV} \\right) = (W^O W^{UV}) \\left( \\sum_j \\alpha_j \\mathbf{c}_j^{KV} \\right)\n",
    "    $$\n",
    "  - 预先计算矩阵 $W^{O'} = W^O W^{UV} \\in \\mathbb{R}^{d \\times d_c}$，则输出可直接从低维加权和计算：\n",
    "    $$\n",
    "    \\mathbf{u}_t = W^{O'} \\left( \\sum_j \\alpha_j \\mathbf{c}_j^{KV} \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acd52c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
