{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664a721b",
   "metadata": {},
   "source": [
    "# 从零构建一个LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa69e53",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "import copy\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9257e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置文件\n",
    "@dataclass\n",
    "class DeepSeekV2Config:\n",
    "    # 基础参数\n",
    "    vocab_size: int = 151936\n",
    "    hidden_size: int = 4096\n",
    "    num_hidden_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    max_position_embeddings: int = 4096\n",
    "    initializer_range: float = 0.02   ###############\n",
    "    max_epochs: int = 100\n",
    "    \n",
    "    # MLA 参数\n",
    "    q_lora_rank: int = 1536\n",
    "    qk_rope_head_dim: int = 64\n",
    "    kv_lora_rank: int = 512\n",
    "    v_head_dim: int = 128\n",
    "    qk_nope_head_dim: int = 128\n",
    "    rope_theta: float = 10000.0\n",
    "    attention_bias: bool = False\n",
    "    \n",
    "    # MoE 参数\n",
    "    expert_number: int = 8\n",
    "    top_k: int = 2\n",
    "    shared_expert_number: int = 2\n",
    "    moe_load_balance_alpha: float = 0.01\n",
    "    expert_dropout: float = 0.1\n",
    "    \n",
    "    # 训练参数\n",
    "    batch_size: int = 4\n",
    "    seq_len: int = 2048\n",
    "    lr: float = 5e-5\n",
    "    weight_decay: float = 0.1   \n",
    "    warmup_steps: int = 1000\n",
    "    total_steps: int = 100000\n",
    "    valid_steps: int = 100\n",
    "    grad_accum_steps: int = 1   \n",
    "    save_every: int = 1000\n",
    "    validation_batch: int = 50      ###\n",
    "    async_validation: bool = True   ###\n",
    "    \n",
    "    # 其他参数\n",
    "    attention_dropout: float = 0.1\n",
    "    hidden_dropout: float = 0.1\n",
    "    gradient_checkpointing: bool = False\n",
    "    tie_word_embeddings: bool = True\n",
    "    output_hidden_states: bool = False\n",
    "    output_attentions: bool = False\n",
    "    output_router_logits: bool = False\n",
    "\n",
    "    # 日志和检查点\n",
    "    log_dir: str = \"model/logs\"\n",
    "    checkpoint_dir: str = \"model/checkpoints\"\n",
    "    experiment_name: str = \"llm_experiment\"\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"保存配置到JSON文件\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(asdict(self), f, indent=4)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"从JSON文件加载配置\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd80c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "class DeepseekV2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "    \n",
    "class DeepseekV2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (\n",
    "            self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings,\n",
    "            device=self.inv_freq.device,\n",
    "            dtype=torch.get_default_dtype(),\n",
    "        )\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(\n",
    "            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype\n",
    "        )\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq.to(t.device))\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len is not None and seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# 旋转位置编码MALA\n",
    "def apply_rotary_pos_emb_v2(q: torch.Tensor, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    b, h, s, d = q.shape\n",
    "    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    return q_embed\n",
    "\n",
    "# 学习率调度器(init_lr=1e-6)\n",
    "class WarmupCosineScheduler:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, init_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        self.init_lr = init_lr\n",
    "        self.base_lr = optimizer.param_groups[0]['lr'] \n",
    "        \n",
    "        # Set initial learning rates\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group.setdefault('initial_lr', group['lr'])\n",
    "        \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self._get_lr()\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def _get_lr(self):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return self.init_lr + (self.base_lr - self.init_lr) * self.current_step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress)) * self.optimizer.param_groups[0]['initial_lr']\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'current_step': self.current_step,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'total_steps': self.total_steps,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.current_step = state_dict['current_step']\n",
    "        self.warmup_steps = state_dict['warmup_steps']\n",
    "        self.total_steps = state_dict['total_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07a0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MoE: expert_hidden_dim = hidden_size * 2~3 (提升空间)\n",
    "class FFNExpert(nn.Module):\n",
    "    def __init__(self, hidden_dim, expert_dropout):\n",
    "        super().__init__()\n",
    "        mid_dim = hidden_dim * 8 // 3\n",
    "\n",
    "        self.up = nn.Linear(hidden_dim, mid_dim, bias=False)\n",
    "        self.down = nn.Linear(mid_dim, hidden_dim, bias=False)\n",
    "        self.gate = nn.Linear(hidden_dim, mid_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(expert_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.dropout(\n",
    "            self.down(\n",
    "                F.silu(self.gate(x)) * self.up(x)\n",
    "            )\n",
    "        )\n",
    "        return output\n",
    "\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_size, config.expert_number)\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        router_logits = self.gate(x)\n",
    "        router_probs = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        \n",
    "        router_weights, selected_expert_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim=-1,\n",
    "        )\n",
    "        \n",
    "        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)\n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "        \n",
    "        expert_mask = F.one_hot(\n",
    "            selected_expert_indices, \n",
    "            num_classes=self.expert_number,\n",
    "        )\n",
    "        \n",
    "        expert_mask = expert_mask.permute(2, 1, 0)\n",
    "        \n",
    "        return router_logits, router_weights, selected_expert_indices, expert_mask, router_probs\n",
    "\n",
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.top_k = config.top_k\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.expert_number = config.expert_number\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [FFNExpert(config.hidden_size, config.expert_dropout) for _ in range(config.expert_number)]\n",
    "        )\n",
    "        self.router = MOERouter(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        hidden_states = x.view(-1, hidden_dim)\n",
    "        \n",
    "        router_logits, router_weights, _, expert_masks, router_probs= self.router(hidden_states)\n",
    "        \n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        for expert_idx in range(self.expert_number):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            current_expert_mask = expert_masks[expert_idx]\n",
    "            \n",
    "            router_weight_idx, top_x = torch.where(current_expert_mask)\n",
    "            \n",
    "            current_states = hidden_states[top_x, :]\n",
    "            current_states = expert_layer(current_states)\n",
    "            \n",
    "            current_token_router_weight = router_weights[top_x, router_weight_idx].unsqueeze(-1)\n",
    "            current_hidden_states = current_states * current_token_router_weight\n",
    "            \n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states)\n",
    "        \n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "        router_logits = router_logits.view(batch_size, seq_len, -1)\n",
    "\n",
    "        return final_hidden_states, router_logits, expert_masks, router_probs  \n",
    "\n",
    "\n",
    "class ShareExpertMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.moe_model = SparseMOE(config)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            FFNExpert(config.hidden_size, config.expert_dropout) \n",
    "            for _ in range(config.shared_expert_number)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        sparse_moe_out, router_logits, expert_masks, router_probs= self.moe_model(x)\n",
    "        \n",
    "        shared_experts_out = torch.stack(\n",
    "            [expert(x) for expert in self.shared_experts], dim=0\n",
    "        ).sum(dim=0)\n",
    "        \n",
    "        moe_loss = self._calculate_moe_loss(expert_masks, router_probs)\n",
    "        \n",
    "        return sparse_moe_out + shared_experts_out, router_logits, moe_loss\n",
    "    \n",
    "\n",
    "    def _calculate_moe_loss(self, expert_masks, router_probs):\n",
    "        \"\"\"\n",
    "        expert_masks: [num_experts, top_k, batch*seq_len]\n",
    "        router_probs: [batch*seq_len, num_experts]\n",
    "        \"\"\"\n",
    "        # importance / router_fraction\n",
    "        router_fraction = router_probs.mean(dim=0)  # [num_experts]\n",
    "\n",
    "        # load / expert_fraction\n",
    "        load = expert_masks.float().mean(dim=[1,2])  # [num_experts]\n",
    "\n",
    "        # load balancing loss\n",
    "        moe_loss = self.config.moe_load_balance_alpha * torch.sum(load * router_fraction)\n",
    "        \n",
    "        return moe_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e492fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLA注意力机制\n",
    "class MLAV2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "\n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "\n",
    "        self.q_down_proj = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.q_lora_rank,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        self.q_down_layernorm = DeepseekV2RMSNorm(self.q_lora_rank)\n",
    "\n",
    "        self.q_up_proj = nn.Linear(\n",
    "            self.q_lora_rank,\n",
    "            self.num_heads * self.q_head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.kv_down_proj = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.kv_lora_rank + self.qk_rope_head_dim,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        self.kv_down_layernorm = DeepseekV2RMSNorm(self.kv_lora_rank)\n",
    "        self.kv_up_proj = nn.Linear(\n",
    "            self.kv_lora_rank,\n",
    "            self.num_heads * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.o_proj = nn.Linear(\n",
    "            self.num_heads * self.v_head_dim,\n",
    "            self.hidden_size,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "\n",
    "        self.rotary_emb = DeepseekV2RotaryEmbedding(\n",
    "            self.qk_rope_head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        compressed_kv: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # Query projection and split\n",
    "        q = self.q_up_proj(self.q_down_layernorm(self.q_down_proj(hidden_states)))\n",
    "        q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "\n",
    "        # Key/Value projection and split\n",
    "        if compressed_kv is None:\n",
    "            compressed_kv = self.kv_down_proj(hidden_states)  # [B, L, kv_lora_rank + qk_rope_head_dim]\n",
    "            raw_kv, k_pe = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "            # 对低秩部分做 layernorm（和你在 init 中定义的一致）\n",
    "            compressed_kv = self.kv_down_layernorm(raw_kv)  # [B, seq, kv_lora_rank]\n",
    "        else:\n",
    "            # 兼容上层传入的合并张量（假设传入的就是 kv_lora_rank + qk_rope_head_dim 的合并形式）\n",
    "            raw_kv, k_pe = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "            compressed_kv = raw_kv  # 不在此处额外做 layernorm（假定上层缓存已处理）\n",
    "\n",
    "        kv_seq_len = compressed_kv.size(1)\n",
    "        k_pe = k_pe.view(bsz, kv_seq_len, 1, self.qk_rope_head_dim).transpose(1, 2)\n",
    "\n",
    "        # Split kv_up_proj into heads\n",
    "        kv_up_proj = self.kv_up_proj.weight.view(self.num_heads, -1, self.kv_lora_rank)\n",
    "        q_absorb = kv_up_proj[:, :self.qk_nope_head_dim, :]\n",
    "        out_absorb = kv_up_proj[:, self.qk_nope_head_dim:, :]\n",
    "\n",
    "        # Apply RoPE\n",
    "        cos, sin = self.rotary_emb(q_pe, seq_len=q_len)\n",
    "        q_pe = apply_rotary_pos_emb_v2(q_pe, cos, sin, position_ids)\n",
    "\n",
    "        # Project q_nope\n",
    "        q_nope = torch.matmul(q_nope, q_absorb)\n",
    "\n",
    "        # Attention score calculation\n",
    "        attn_weights = (\n",
    "            torch.matmul(q_pe, k_pe.mT)\n",
    "            + torch.matmul(q_nope, compressed_kv.unsqueeze(-3).mT)\n",
    "        ) / math.sqrt(self.q_head_dim)\n",
    "\n",
    "        # Apply causal mask（合并causal mask和padding mask） \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [bsz, 1, 1, seq_len]\n",
    "            attention_mask = attention_mask.expand(-1, self.num_heads, q_len, -1)\n",
    "            # 创建causal mask\n",
    "            causal_mask = torch.tril(\n",
    "                torch.ones((q_len, kv_seq_len), device=attn_weights.device, dtype=torch.bool)\n",
    "            )\n",
    "            causal_mask = causal_mask.view(1, 1, q_len, kv_seq_len)\n",
    "            attn_weights = attn_weights.masked_fill(~causal_mask, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = nn.functional.softmax(\n",
    "            attn_weights, dim=-1, dtype=torch.float32\n",
    "        ).to(q_nope.dtype)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.einsum(\"bhql,blc->bhqc\", attn_weights, compressed_kv)\n",
    "        attn_output = torch.matmul(attn_output, out_absorb.mT)\n",
    "\n",
    "        # Merge heads and project\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(bsz, q_len, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe30196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Pre-LN architecture\n",
    "        self.ln1 = DeepseekV2RMSNorm(config.hidden_size)\n",
    "        self.attn = MLAV2(config)\n",
    "        self.ln2 = DeepseekV2RMSNorm(config.hidden_size)\n",
    "        self.moe = ShareExpertMOE(config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        compressed_kv: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_router_logits: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        # Self Attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln1(hidden_states)\n",
    "        attn_output, attn_weights = self.attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            # compressed_kv=compressed_kv,\n",
    "        )\n",
    "        hidden_states = residual + self.dropout(attn_output)\n",
    "\n",
    "        # MoE FFN\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln2(hidden_states)\n",
    "        ffn_output, router_logits, moe_loss = self.moe(hidden_states)  \n",
    "        hidden_states = residual + self.dropout(ffn_output)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "        if output_router_logits:\n",
    "            outputs += (router_logits,)\n",
    "            \n",
    "        return outputs, moe_loss, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d2a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekV2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.gradient_checkpointing = config.gradient_checkpointing ## 梯度检查点\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.embed_positions = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = DeepseekV2RMSNorm(config.hidden_size)\n",
    "        \n",
    "        if config.tie_word_embeddings:\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            self.lm_head.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            \n",
    "        self.gradient_checkpointing = False\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 1.0 / math.sqrt(module.weight.size(1))  # hidden_size 自适应\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            std = 1.0 / math.sqrt(module.weight.size(1))\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "        elif isinstance(module, DeepseekV2RMSNorm):\n",
    "            nn.init.ones_(module.weight) \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        output_router_logits = output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "\n",
    "        # Prepare inputs\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, seq_length, dtype=torch.long, device=device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones((batch_size, seq_length), device=device)\n",
    "        \n",
    "        # Embed positions and tokens\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        position_embeds = self.embed_positions(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        \n",
    "        # Initialize output containers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_router_logits = () if output_router_logits else None\n",
    "\n",
    "        # Forward through layers\n",
    "        for layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs, moe_loss, router_logits = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                output_attentions=output_attentions,\n",
    "                output_router_logits=output_router_logits,\n",
    "            )\n",
    "            \n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                \n",
    "            if output_router_logits:\n",
    "                all_router_logits = all_router_logits + (router_logits,)\n",
    "        \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            \n",
    "        # Compute logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"moe_loss\": moe_loss,\n",
    "            \"hidden_states\": all_hidden_states,\n",
    "            \"attentions\": all_self_attentions,\n",
    "            \"router_logits\": all_router_logits,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        top_p: float = None,\n",
    "        eos_token_id: int = None,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Decoder-only 自回归生成\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = input_ids.device\n",
    "        output_ids = input_ids.clone()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # forward 获取 logits\n",
    "            logits = self.forward(output_ids)[\"logits\"]  # [B, seq_len, vocab]\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # top-k 过滤\n",
    "            if top_k is not None:\n",
    "                topk_vals, topk_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                mask = next_token_logits < topk_vals[:, -1][:, None]\n",
    "                next_token_logits = next_token_logits.masked_fill(mask, -float('Inf'))\n",
    "\n",
    "            # top-p 过滤\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "                next_token_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
    "\n",
    "            # 采样下一个 token\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [B, 1]\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
    "\n",
    "            # 如果 EOS 出现则停止\n",
    "            if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "                break\n",
    "\n",
    "        return output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff74d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练器\n",
    "class LLMTrainer:\n",
    "    def __init__(self, model, config, train_dataset, valid_dataset=None):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        # 数据加载器\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.config.batch_size, \n",
    "            shuffle=True, \n",
    "            collate_fn=self.collate_fn,\n",
    "            pin_memory=True   # CPU->GPU 传输更快\n",
    "        )\n",
    "\n",
    "        if self.valid_dataset is not None:\n",
    "            self.valid_loader = DataLoader(\n",
    "                self.valid_dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False,   # 验证集不需要 shuffle\n",
    "                collate_fn=self.collate_fn,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        else:\n",
    "            self.valid_loader = None\n",
    "\n",
    "        self.validation_executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)\n",
    "        self.validation_future = None\n",
    "        self.last_validation_result = None\n",
    "\n",
    "        self.val_loss_ema = None  \n",
    "        self.ema_decay = 0.9      # 衰减系数，可以调\n",
    "    \n",
    "\n",
    "        # 设备设置\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # 优化器和学习率调度器\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config.lr, \n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = WarmupCosineScheduler(\n",
    "            self.optimizer, \n",
    "            config.warmup_steps, \n",
    "            config.total_steps\n",
    "        )\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # TensorBoard 记录器\n",
    "        log_dir = os.path.join(config.log_dir, config.experiment_name)\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        \n",
    "        # 训练状态\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask = batch  # 解包batch\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = self.model(input_ids, \n",
    "                                 attention_mask, \n",
    "                                 output_router_logits=True)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # 计算语言建模损失\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            lm_loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # 添加MoE负载均衡损失\n",
    "            moe_loss = outputs.get(\"moe_loss\", 0.0)\n",
    "            if isinstance(moe_loss, float):\n",
    "                moe_loss = torch.tensor(moe_loss, device=self.device)\n",
    "\n",
    "            total_loss = lm_loss + moe_loss\n",
    "\n",
    "        # 专家使用率\n",
    "        with torch.no_grad():\n",
    "            router_logits = outputs.get(\"router_logits\", None)\n",
    "\n",
    "            if router_logits is not None and len(router_logits) > 0:\n",
    "                # 取最后一层的 router logits\n",
    "                last_router_logits = router_logits[-1]  # shape: [batch, seq, num_experts]\n",
    "                router_probs = torch.softmax(last_router_logits, dim=-1)\n",
    "\n",
    "                # 专家使用率\n",
    "                expert_usage = router_probs.mean(dim=[0, 1])  # [num_experts]\n",
    "                for i, usage in enumerate(expert_usage):\n",
    "                    self.writer.add_scalar(f'router/expert_{i}_usage', usage.item(), self.global_step)\n",
    "\n",
    "                # 熵\n",
    "                entropy = -torch.sum(router_probs * torch.log(router_probs + 1e-8), dim=-1).mean()\n",
    "                self.writer.add_scalar('router/entropy', entropy.item(), self.global_step)\n",
    "            else:\n",
    "                # 没有 router logits 的情况\n",
    "                self.writer.add_scalar('router/entropy', 0.0, self.global_step)\n",
    "\n",
    "        \n",
    "        #反向传播\n",
    "        self.scaler.scale(total_loss).backward()\n",
    "        \n",
    "        # 记录损失到 TensorBoard\n",
    "        self.writer.add_scalar('train/lm_loss', lm_loss.item(), self.global_step)\n",
    "        self.writer.add_scalar('train/moe_loss', moe_loss.item(), self.global_step)\n",
    "        self.writer.add_scalar('train/total_loss', total_loss.item() * self.config.grad_accum_steps, self.global_step)\n",
    "        self.writer.add_scalar('train/learning_rate', self.optimizer.param_groups[0]['lr'], self.global_step)\n",
    "        \n",
    "        # 梯度累积步骤\n",
    "        if (self.global_step + 1) % self.config.grad_accum_steps == 0:\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(),  max_norm=1.0)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "        # 监控数值稳定性\n",
    "        if self.global_step % 100 == 0:\n",
    "            # 检查激活值范围\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    grad_mean = param.grad.abs().mean().item()\n",
    "                    if grad_mean > 1e3:  # 梯度爆炸\n",
    "                        print(f\"Warning: Large gradients in {name}: {grad_mean:.6f}\")\n",
    "            \n",
    "            # 检查损失数值\n",
    "            if total_loss.item() > 100:  # 损失异常高\n",
    "                print(f\"Warning: High loss {total_loss.item():.6f}\")\n",
    "        \n",
    "        return {\n",
    "            \"lm_loss\": lm_loss.item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"moe_loss\": moe_loss.item()\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "            # train_dataset is dict，cloumns 'input_ids' and 'attention_mask'\n",
    "            input_ids_list = [item['input_ids'] for item in batch]\n",
    "            attention_mask_list = [item['attention_mask'] for item in batch]\n",
    "            \n",
    "            # 计算最大长度，不超过config.seq_len\n",
    "            max_len = min(max([len(ids) for ids in input_ids_list]), self.config.seq_len)\n",
    "            \n",
    "            # 初始化填充后的张量\n",
    "            padded_input_ids = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
    "            padded_attention_mask = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
    "            \n",
    "            for i, (ids, mask) in enumerate(zip(input_ids_list, attention_mask_list)):\n",
    "                l = min(len(ids), max_len)\n",
    "                padded_input_ids[i, :l] = torch.tensor(ids[:l], dtype=torch.long)\n",
    "                padded_attention_mask[i, :l] = torch.tensor(mask[:l], dtype=torch.long)\n",
    "            \n",
    "            return padded_input_ids, padded_attention_mask\n",
    "    \n",
    "    def get_validation_batch(self):\n",
    "        \"\"\"根据训练进度动态控制验证 batch 数\"\"\"\n",
    "        progress = self.global_step / self.config.total_steps\n",
    "        validation_batch = self.config.validation_batch\n",
    "        \n",
    "        if progress < 0.3:\n",
    "            return max(1, int(validation_batch * 0.15))   # 前期少验证\n",
    "        elif progress < 0.7:\n",
    "            return max(1, int(validation_batch * 0.4))  # 中期适中\n",
    "        else:\n",
    "            return validation_batch  # 后期更稳定\n",
    "        \n",
    "\n",
    "    def update_val_loss_ema(self, new_val_loss):\n",
    "        \"\"\"更新指数滑动平均\"\"\"\n",
    "        if self.val_loss_ema is None:\n",
    "            self.val_loss_ema = new_val_loss\n",
    "        else:\n",
    "            self.val_loss_ema = (\n",
    "                self.ema_decay * self.val_loss_ema\n",
    "                + (1 - self.ema_decay) * new_val_loss\n",
    "            )\n",
    "        return self.val_loss_ema\n",
    "    \n",
    "    def async_validate(self):\n",
    "        \"\"\"异步执行检测\"\"\"\n",
    "        if self.valid_dataset is None:\n",
    "            return None\n",
    "\n",
    "        # 深度拷贝\n",
    "        model_copy = copy.deepcopy(self.model)\n",
    "        model_copy.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_moe_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "\n",
    "        max_batch = self.get_validation_batch()\n",
    "\n",
    "        with torch.no_grad():\n",
    "             for i, (input_ids, attention_mask) in enumerate(self.valid_loader):\n",
    "                if i >= max_batch:  # 限制验证批次\n",
    "                    break\n",
    "\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(input_ids, \n",
    "                                         attention_mask, \n",
    "                                         output_router_logits=True)\n",
    "                    logits = outputs[\"logits\"]\n",
    "                    \n",
    "                    # 计算LM损失\n",
    "                    shift_logits = logits[:, :-1, :].contiguous()\n",
    "                    shift_labels = input_ids[:, 1:].contiguous()\n",
    "                    loss = F.cross_entropy(\n",
    "                        shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "\n",
    "                    # 获取MoE损失\n",
    "                    moe_loss = outputs.get(\"moe_loss\", 0.0)\n",
    "                    if isinstance(moe_loss, float):\n",
    "                        moe_loss = torch.tensor(moe_loss, device=self.device)\n",
    "                \n",
    "                total_loss += loss.item() * input_ids.size(0)\n",
    "                total_moe_loss += moe_loss.item() * input_ids.size(0)\n",
    "                total_samples += input_ids.size(0)\n",
    "        \n",
    "        avg_lm_loss = total_loss / total_samples\n",
    "        avg_moe_loss = total_moe_loss / total_samples\n",
    "        avg_total_loss = avg_lm_loss + avg_moe_loss\n",
    "        ema_loss = self.update_val_loss_ema(avg_total_loss)\n",
    "\n",
    "        self.writer.add_scalar('val/lm_loss', avg_lm_loss, self.global_step)\n",
    "        self.writer.add_scalar('val/moe_loss', avg_moe_loss, self.global_step)\n",
    "        self.writer.add_scalar('val/total_loss', avg_total_loss, self.global_step)\n",
    "        self.writer.add_scalar('val/ema_loss', ema_loss, self.global_step)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if ema_loss < self.best_val_loss:\n",
    "            self.best_val_loss = ema_loss\n",
    "            self.save_checkpoint(f\"best_model.pt\")\n",
    "        \n",
    "        return avg_total_loss\n",
    "    \n",
    "    def check_validation_result(self):\n",
    "        \"\"\"检查异步验证结果\"\"\"\n",
    "        if self.validation_future and self.validation_future.done():\n",
    "            try:\n",
    "                val_loss = self.validation_future.result()\n",
    "                self.last_validation_result = val_loss\n",
    "                self.writer.add_scalar('val/loss', val_loss, self.global_step)\n",
    "                \n",
    "                # 保存最佳模型\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.save_checkpoint(f\"best_model.pt\")\n",
    "                    \n",
    "                # tqdm.write(f\"Step {self.global_step}: val_loss = {val_loss:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Validation error: {e}\")\n",
    "            finally:\n",
    "                self.validation_future = None\n",
    "\n",
    "    def train(self):\n",
    "        # 训练循环\n",
    "        progress_bar = tqdm(total=self.config.total_steps, desc=\"Training\")\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):  # 足够大的epoch数，通过total_steps控制\n",
    "            for batch in self.train_loader:\n",
    "                if self.global_step >= self.config.total_steps:\n",
    "                    break\n",
    "                \n",
    "                # 检查是否有验证结果\n",
    "                self.check_validation_result()\n",
    "\n",
    "                # 训练步骤\n",
    "                metrics = self.train_step(batch)\n",
    "                \n",
    "                # 更新进度条\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{metrics['total_loss']:.4f}\",\n",
    "                    \"lr\": f\"{self.optimizer.param_groups[0]['lr']:.2e}\",\n",
    "                    \"val_loss\": f\"{self.last_validation_result:.4f}\" if self.last_validation_result else \"N/A\"\n",
    "                })\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # 验证和保存检查点\n",
    "                if (self.global_step % self.config.valid_steps == 0 \n",
    "                    and self.validation_future is None):\n",
    "                    self.validation_future = self.validation_executor.submit(self.async_validate)\n",
    "\n",
    "                    # if val_loss is not None:\n",
    "                    #     tqdm.write(f\"Step {self.global_step}: val_loss = {val_loss:.4f}\")\n",
    "                \n",
    "                if self.global_step % self.config.save_every == 0:\n",
    "                    self.save_checkpoint(f\"checkpoint_step_{self.global_step}.pt\")\n",
    "                \n",
    "                self.global_step += 1\n",
    "            \n",
    "            if self.global_step >= self.config.total_steps:\n",
    "                break\n",
    "        # 等待最后的验证完成\n",
    "        if self.validation_future:\n",
    "            self.validation_future.result()\n",
    "        \n",
    "        progress_bar.close()\n",
    "        self.writer.close()\n",
    "        \n",
    "        # 保存最终模型\n",
    "        self.save_checkpoint(\"final_model.pt\")\n",
    "    \n",
    "    def save_checkpoint(self, filename):\n",
    "        checkpoint_path = os.path.join(self.config.checkpoint_dir, self.config.experiment_name, filename)\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'global_step': self.global_step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'scaler_state_dict': self.scaler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': asdict(self.config)\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        self.global_step = checkpoint['global_step']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fc88f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepSeekV2Model(\n",
       "  (embed_tokens): Embedding(151643, 512)\n",
       "  (embed_positions): Embedding(512, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (ln1): DeepseekV2RMSNorm()\n",
       "      (attn): MLAV2(\n",
       "        (q_down_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "        (q_down_layernorm): DeepseekV2RMSNorm()\n",
       "        (q_up_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (kv_down_proj): Linear(in_features=512, out_features=144, bias=False)\n",
       "        (kv_down_layernorm): DeepseekV2RMSNorm()\n",
       "        (kv_up_proj): Linear(in_features=96, out_features=704, bias=False)\n",
       "        (o_proj): Linear(in_features=384, out_features=512, bias=False)\n",
       "        (rotary_emb): DeepseekV2RotaryEmbedding()\n",
       "      )\n",
       "      (ln2): DeepseekV2RMSNorm()\n",
       "      (moe): ShareExpertMOE(\n",
       "        (moe_model): SparseMOE(\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x FFNExpert(\n",
       "              (up): Linear(in_features=512, out_features=1365, bias=False)\n",
       "              (down): Linear(in_features=1365, out_features=512, bias=False)\n",
       "              (gate): Linear(in_features=512, out_features=1365, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (router): MOERouter(\n",
       "            (gate): Linear(in_features=512, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (shared_experts): ModuleList(\n",
       "          (0-1): 2 x FFNExpert(\n",
       "            (up): Linear(in_features=512, out_features=1365, bias=False)\n",
       "            (down): Linear(in_features=1365, out_features=512, bias=False)\n",
       "            (gate): Linear(in_features=512, out_features=1365, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): DeepseekV2RMSNorm()\n",
       "  (lm_head): Linear(in_features=512, out_features=151643, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DeepSeekV2Config(\n",
    "    # 基础参数（极小化）\n",
    "    vocab_size = 151643,            # Qwen的词表\n",
    "    hidden_size = 512,              # embedding 维度设置 MLA里\n",
    "    num_hidden_layers = 4,          # 仅 4 层\n",
    "    num_attention_heads = 4,        # 4 个注意力头\n",
    "    max_position_embeddings = 512,\n",
    "    initializer_range = 0.02,\n",
    "    # max_epochs = 100, deafult\n",
    "\n",
    "    # MLA 参数（极小化）\n",
    "    q_lora_rank = 128,      # 原64 → 128\n",
    "    kv_lora_rank = 96,      # 原32 → 96  \n",
    "    qk_rope_head_dim = 48,  # 原32 → 48\n",
    "    qk_nope_head_dim = 80,  # 原64 → 80\n",
    "    v_head_dim = 96,        # 原64 → 96\n",
    "    rope_theta = 10000.0,\n",
    "    attention_bias = False,\n",
    "\n",
    "    # MoE 参数（小规模测试）\n",
    "    expert_number = 8,          # 仅 2 个专家\n",
    "    top_k = 2,                  # 每次只激活 1 个\n",
    "    shared_expert_number = 2,\n",
    "    moe_load_balance_alpha= 0.01,\n",
    "    expert_dropout = 0.1,\n",
    "\n",
    "    # 训练参数（快速实验）\n",
    "    batch_size = 4,\n",
    "    seq_len = 512,              \n",
    "    lr = 1e-5,\n",
    "    weight_decay = 0.1,         \n",
    "    warmup_steps = 150,\n",
    "    total_steps = 10000,  \n",
    "    save_every = 10000,\n",
    "    grad_accum_steps=1,         # 梯度累计关闭，有可能对梯度爆炸\n",
    "    valid_steps = 150,      \n",
    "    validation_batch=50,\n",
    "    async_validation=True,    \n",
    "\n",
    "    # 其他参数\n",
    "    attention_dropout = 0.05,\n",
    "    hidden_dropout = 0.1,\n",
    "    tie_word_embeddings = True,\n",
    "    output_hidden_states = False,\n",
    "    output_attentions = False,\n",
    "    output_router_logits = True,\n",
    "\n",
    "    # 日志和检查点\n",
    "    log_dir = \".model/logs\",\n",
    "    checkpoint_dir= \".model/checkpoints\",\n",
    "    experiment_name = \"llm_experiment_8\",\n",
    ")\n",
    "model = DeepSeekV2Model(config)\n",
    "model.apply(model._init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "025d4b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0041, -0.0245,  0.0615,  0.0088, -0.0512],\n",
      "        [ 0.0228,  0.1038,  0.0265, -0.0415,  0.0258],\n",
      "        [-0.0488,  0.0112,  0.0542, -0.0627,  0.0016],\n",
      "        [-0.0491, -0.0184,  0.0093, -0.0004,  0.0613],\n",
      "        [-0.0326,  0.0582, -0.0642, -0.0075, -0.0313]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0266,  0.0228,  0.0310,  ...,  0.0371, -0.0277,  0.0614],\n",
      "        [-0.0436,  0.0076,  0.0081,  ..., -0.0363, -0.0021,  0.0378],\n",
      "        [ 0.0388,  0.0363,  0.0011,  ..., -0.0297, -0.0462,  0.0007],\n",
      "        ...,\n",
      "        [-0.0209,  0.0363,  0.0619,  ...,  0.0100,  0.0542,  0.0601],\n",
      "        [-0.0785,  0.0399,  0.0161,  ...,  0.0688,  0.0553,  0.0214],\n",
      "        [-0.0511, -0.0298, -0.0242,  ..., -0.1000, -0.0659,  0.0278]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.embed_tokens.weight[:5, :5])  # 打印前5行5列\n",
    "print(model.layers[0].moe.moe_model.experts[0].gate.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc6818ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总参数数量: 163,666,848\n",
      "可训练参数数量: 163,666,848\n",
      "参数数量 (百万): 163.67M\n"
     ]
    }
   ],
   "source": [
    "# 计算参数数量\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(f\"模型总参数数量: {total_params:,}\")\n",
    "print(f\"可训练参数数量: {trainable_params:,}\")\n",
    "print(f\"参数数量 (百万): {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a52ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\env_LLM\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 234733\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"parquet\",\n",
    "                        data_files = \"./dataset/wudao/clean_weight.parquet\", \n",
    "                        split=\"train\") #.select(range(50))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4ad152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 234733\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.remove_columns(column_names=\"labels\" )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a31ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 234733\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "config.vocab_size = tokenizer.vocab_size  \n",
    "\n",
    "# 把文本转为 token ids\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "    )\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1eac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 211259\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 23474\n",
       " }))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valld_data = tokenized_datasets.train_test_split(test_size=0.1, seed=42).values()\n",
    "train_data, valld_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513c9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LLMTrainer(model, config, train_data, valld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b336685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████| 10000/10000 [3:28:55<00:00,  1.25s/it, loss=7.7800, lr=0.00e+00, val_loss=6.3903]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e4704",
   "metadata": {},
   "source": [
    "![LLMs_loss](./img/LLMs_loss.png)\n",
    "![LLMs_entropy](./img/LLMs_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0619a8a",
   "metadata": {},
   "source": [
    "# 模型生成预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7dca75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(model: nn.Module, checkpoint_path: str, device='cuda'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2016683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我为江湖\\n《我为江湖》是毁灭ko魔神创作的网络小说，发表于起点网。作品简介 初入江湖 1.炼血宗 海浪猛烈的拍击着岸边,远处海鸟在海面上不停的盘旋着,在岸边一个青衣老者和三个长像怪异的人对峙着,那个老者”唉”了一声打破了平静,本来那老者在海边的岩洞里闭关修练,马上可以突破这一层,可是就在这时,跑出眼前的三个人,一句话也没说就开打,要是平常的时侯在来三个也是没问题的,可是在练攻最关键的时侯,为了接住他们的攻击只好强行从入定中醒来,受了很重的内伤. “你们想干什么”青衣老者出声问到. “嘿…嘿…”三个人阴阴笑着. “ [1]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(tokenized_datasets[58845][\"input_ids\"], dtype=torch.long)\n",
    "print(inputs.dtype)\n",
    "tokenizer.decode(inputs, skip_special_token = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "761834b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成结果: 今天的天气如何？站血爱中了解韩\n",
      "新十二阶段肌肤快速的 �青先生,他的一些的:速\",时八,我想以上、甲或者因,我们体\",他在\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你有 tokenizer 和 model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "model = DeepSeekV2Model(config)\n",
    "model = load_pretrained(model, \".model/checkpoints/llm_experiment_5/final_model.pt\", device)\n",
    "\n",
    "# 初始 prompt\n",
    "prompt_text = '今天的天气如何？'\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 自回归生成\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=40,\n",
    "    temperature=0.9,\n",
    "    # top_k=40,\n",
    "    top_p=0.8,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 解码\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"生成结果:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
