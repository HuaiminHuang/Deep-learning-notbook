{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571ce41d-5b67-4dde-a490-267a43bb39a1",
   "metadata": {},
   "source": [
    "# 为什么需要卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bab60-14b2-44a2-8b02-417900012d5d",
   "metadata": {},
   "source": [
    "## 不变性\n",
    "神经网络是将不变性系统化，使用较少的参数来学习有用的表示。  \n",
    "其中的不变性：  \n",
    "1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。\n",
    "2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。\n",
    "\n",
    "![沃尔多游戏示例图。](./img/where-wally-walker-books.jpg)\n",
    ":width:`400px`\n",
    ":label:`img_waldo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19197d-4ad7-4c76-8e3a-4894777323ee",
   "metadata": {},
   "source": [
    "输入一个二维图像**X**其隐藏层**H**表示为一个矩阵（二维张量）且形状相同。  \n",
    "为了使隐藏神经元都能接收到像素信息，我们将参数权重替换为四阶张量**W**:\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "里面的**W到V只是形式长得转换**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7a5dc-fe2a-4ae8-ba20-c3adc945bf0b",
   "metadata": {},
   "source": [
    "*平移不变性*：输入对象**X**平移，对应的隐藏层**H**也会发生平移。也就是说，$\\mathsf{V}$和$\\mathbf{U}$实际上不依赖于$(i, j)$的值，即$[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$。并且$\\mathbf{U}$是一个常数，比如$u$。因此，我们可以简化$\\mathbf{H}$定义为：\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf6cf7-7600-437e-b891-da59254455f3",
   "metadata": {},
   "source": [
    "*局部性*：为了收集用来训练参数$[\\mathbf{H}]_{i, j}$信息，不应偏离到距$(i, j)$很远的地方。这意味着在$|a|> \\Delta$或$|b| > \\Delta$的范围之外，我们可以设置$[\\mathbf{V}]_{a, b} = 0$。因此，我们可以将$[\\mathbf{H}]_{i, j}$重写为\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cb45d-0eeb-44e2-bd62-fdebc5d433a8",
   "metadata": {},
   "source": [
    "为了支持输入$\\mathsf{X}$和隐藏表示$\\mathsf{H}$中的多个通道，我们可以在$\\mathsf{V}$中添加第三、四个坐标，即$[\\mathsf{V}]_{a, b, c, d}$。综上所述，\n",
    "\n",
    "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e30a3e-5f83-46c2-b45e-98038bcee8c1",
   "metadata": {},
   "source": [
    "# 卷积层（conv-layer）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c314da-4476-4954-bd06-bbe406951271",
   "metadata": {},
   "source": [
    "## 互相关运算（cross-correlation）\n",
    "实际表达的运算为互相关运算。  \n",
    "输出大小：\n",
    "$$ (n_k - k_h + 1) \\times(n_w - k_w + 1) $$\n",
    "\n",
    "![二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$.](./img/correlation.svg)\n",
    "\n",
    "例如：$0 \\times 0 + 1 \\times 1 +2 \\times 2 + 3 \\times 3 =  19$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83e731a-db20-4d9c-a1b2-f3af7cb076a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现代码\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911525ab-4a0e-471e-8684-388201c5a459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复现上面的卷积操作\n",
    "X = torch.arange(9).reshape(3, 3)\n",
    "K = torch.arange(4).reshape(2, 2)\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb42938-f0bc-4714-8690-a297e02b9908",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "输入与卷积核进行互相关运算，并且添加怕偏置量后输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b317cb9d-c0d8-4ae7-bb89-435308e0cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size)) # 随机初始化权重\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803754a-e5aa-45bd-b4af-cd9526f0061c",
   "metadata": {},
   "source": [
    "高度和宽度分别为$h$和$w$的卷积核可以被称为$h \\times w$卷积或$h \\times w$卷积核。\n",
    "我们也将带有$h \\times w$卷积核的卷积层称为$h \\times w$卷积层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63387f44-7b91-4e37-989c-2862d3d9ccf4",
   "metadata": {},
   "source": [
    "## 学习卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fa64a7-ef8a-47e3-9eb4-fa7587efb983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4f1884-01ea-40a5-a9d6-7bddcdd7b9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 5.469\n",
      "epoch 4, loss 1.646\n",
      "epoch 6, loss 0.549\n",
      "epoch 8, loss 0.183\n",
      "epoch 10, loss 0.061\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 2e-2  # 学习率\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2b9938-910a-4880-bed0-6e18be1c2a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.9458, -0.9458]]]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4aa1f-f22d-4dd6-9264-fa29c29d4b9f",
   "metadata": {},
   "source": [
    "# 通道（channel）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e583b-e47b-4cf7-82a5-94af0bd29aa7",
   "metadata": {},
   "source": [
    "## 多输入通道\n",
    "在 :numref:`fig_conv_multi_in`中，我们演示了一个具有两个输入通道的二维互相关运算的示例。阴影部分是第一个输出元素以及用于计算这个输出的输入和核张量元素：$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$。\n",
    "\n",
    "![两个输入通道的互相关计算。](./img/conv-multi-in.svg)\n",
    ":label:`fig_conv_multi_in`\n",
    "\n",
    "为了加深理解，我们(**实现一下多输入通道互相关运算**)。\n",
    "简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e49ec48-70b0-4c7a-bba4-c2c79ecd1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20f29d7-2a05-4c77-aeda-77c99a09f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa1b33-073b-4855-b3a7-10d307edc683",
   "metadata": {},
   "source": [
    "## 多输出通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981cc3e1-5b85-4222-b6d8-1b7705987080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起--在第零维堆叠stack\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff41324-4977-4fcd-bd2f-2b37da824bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6235f89-12b3-4d96-abf2-cbf521c187b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22dfbd-3160-44a1-a8d3-7d9d526e75b3",
   "metadata": {},
   "source": [
    "# 1$\\times$1卷积层 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ba0b1-b60a-463b-843a-e8db294ab80b",
   "metadata": {},
   "source": [
    "这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。\n",
    "我们可以将$1\\times 1$卷积层看作在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。\n",
    "因为这仍然是一个卷积层，所以跨像素的权重是一致的。\n",
    "同时，$1\\times 1$卷积层需要的权重维度为$c_o\\times c_i$，再额外加上一个偏置。\n",
    "\n",
    "![互相关计算使用了具有3个输入通道和2个输出通道的 $1\\times 1$ 卷积核。其中，输入和输出具有相同的高度和宽度。](./img/conv-1x1.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16a2ca5-2d48-4082-8062-76ba1db34cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    # 全连接层中的矩阵乘法\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dcd2bfd-d4bf-4019-b3f2-10c469963fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba685309-0452-40ba-9275-e8477b0d5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0874b-403c-45a9-8ad8-1386fe5710ef",
   "metadata": {},
   "source": [
    "# 池化层（pooling）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be494430-01ca-4a7e-af8c-c3a553649001",
   "metadata": {},
   "source": [
    "目的：降低卷积对位置的敏感，同时降低空间采样的敏感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e56c0-42a1-4c83-bbce-0ef5d5f352be",
   "metadata": {},
   "source": [
    "![汇聚窗口形状为 $2\\times 2$ 的最大汇聚层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素: $\\max(0, 1, 3, 4)=4$.](./img/pooling.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5509e884-a8ea-4ed6-96bd-5bc01ab11dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81ff3ef3-5891-48e8-b608-4aa6febc5000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大池化\n",
    "X = torch.arange(9,dtype = torch.float32).reshape((3, 3))\n",
    "pool2d(X, (2, 2), mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f9ec5ea-1bf3-4b39-ac59-64d16421f440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均池化\n",
    "pool2d(X, (2, 2), mode = 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71033de0-a39c-44fe-b5d3-1f7f708af5a1",
   "metadata": {},
   "source": [
    "## [**填充和步幅**]\n",
    "\n",
    "与卷积层一样，汇聚层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。\n",
    "下面，我们用深度学习框架中内置的二维最大汇聚层，来演示汇聚层中填充和步幅的使用。\n",
    "我们首先构造了一个输入张量`X`，它有四个维度，其中样本数和通道数都是1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99f8495-5f2a-4bbc-9ea3-7ab862c9abf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype = torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c924b647-dcb7-4fe6-9564-b244bb38a36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认和汇池化口一样\n",
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28eb108a-cc12-426a-8ed5-374ea643781e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动设置填充步幅\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b14f7b9-7610-4bbd-a84d-50943fa48af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以自定义任意大小的池化窗口（高，宽）\n",
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3437b-0a95-419d-814a-3819bfb05937",
   "metadata": {},
   "source": [
    "## 多通道情形\n",
    "在处理多通道输入数据时，[**汇聚层在每个输入通道上单独运算**]，而不是像卷积层一样在通道上对输入进行汇总。\n",
    "这意味着汇聚层的输出通道数与输入通道数相同。\n",
    "下面，我们将在通道维度上连结张量`X`和`X + 1`，以构建具有2个通道的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f1b54bb-e84f-4bab-88cd-ddb884bedce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61fdf2ac-18ec-45cb-9d57-d22ffe9e7894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c0874-1309-4db4-9818-144f851bf02a",
   "metadata": {},
   "source": [
    "输出通道还是2，可见不改变通道数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
